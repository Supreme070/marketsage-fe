{"version":3,"sources":["/Users/supreme/Desktop/marketsage/src/lib/ai/automl-engine.ts"],"sourcesContent":["/**\n * Supreme-AI AutoML Engine\n * ========================\n * Automated Machine Learning for optimal model selection and hyperparameter tuning\n * \n * Features:\n * ðŸ¤– Auto-hyperparameter tuning\n * ðŸ“Š Multiple algorithm comparison  \n * ðŸŽ¯ Ensemble model creation\n * ðŸ“ˆ Performance tracking\n * ðŸ”„ Auto-retraining triggers\n */\n\nimport { logger } from '@/lib/logger';\n\ninterface ModelConfig {\n  algorithm: 'linear' | 'tree' | 'ensemble' | 'neural';\n  hyperparams: Record<string, number>;\n  performance: number;\n  trainTime: number;\n}\n\ninterface AutoMLResult {\n  bestModel: ModelConfig;\n  allModels: ModelConfig[];\n  improvementPercent: number;\n  confidence: number;\n}\n\nexport class AutoMLEngine {\n  private modelHistory: ModelConfig[] = [];\n  private bestPerformance = 0;\n\n  // Simplified Linear Regression\n  private linearRegression(features: number[][], targets: number[], params: any) {\n    const { learningRate = 0.01, iterations = 1000 } = params;\n    \n    const m = features.length;\n    const n = features[0].length;\n    const weights = Array(n).fill(0);\n    let bias = 0;\n    let totalLoss = 0;\n    \n    for (let iter = 0; iter < iterations; iter++) {\n      const weightGradients = Array(n).fill(0);\n      let biasGradient = 0;\n      totalLoss = 0;\n      \n      for (let i = 0; i < m; i++) {\n        const prediction = features[i].reduce((sum, feature, j) => sum + feature * weights[j], bias);\n        const error = prediction - targets[i];\n        totalLoss += error * error;\n        \n        for (let j = 0; j < n; j++) {\n          weightGradients[j] += (2 / m) * error * features[i][j];\n        }\n        biasGradient += (2 / m) * error;\n      }\n      \n      for (let j = 0; j < n; j++) {\n        weights[j] -= learningRate * weightGradients[j];\n      }\n      bias -= learningRate * biasGradient;\n    }\n    \n    return { weights, bias, loss: Math.sqrt(totalLoss / m) };\n  }\n\n  // Decision Tree (simplified)\n  private decisionTree(features: number[][], targets: number[], params: any) {\n    const { maxDepth = 5, minSamplesLeaf = 2 } = params;\n    \n    const buildTree = (indices: number[], depth: number): any => {\n      if (depth >= maxDepth || indices.length <= minSamplesLeaf) {\n        const avg = indices.reduce((sum, i) => sum + targets[i], 0) / indices.length;\n        return { value: avg, isLeaf: true };\n      }\n      \n      let bestSplit = { feature: 0, threshold: 0, gain: Number.NEGATIVE_INFINITY };\n      \n      for (let featureIdx = 0; featureIdx < features[0].length; featureIdx++) {\n        const values = indices.map(i => features[i][featureIdx]).sort((a, b) => a - b);\n        \n        for (let i = 1; i < values.length; i++) {\n          const threshold = (values[i] + values[i-1]) / 2;\n          const leftIndices = indices.filter(idx => features[idx][featureIdx] <= threshold);\n          const rightIndices = indices.filter(idx => features[idx][featureIdx] > threshold);\n          \n          if (leftIndices.length === 0 || rightIndices.length === 0) continue;\n          \n          const leftMean = leftIndices.reduce((sum, idx) => sum + targets[idx], 0) / leftIndices.length;\n          const rightMean = rightIndices.reduce((sum, idx) => sum + targets[idx], 0) / rightIndices.length;\n          \n          const variance = indices.reduce((sum, idx) => {\n            const target = targets[idx];\n            const mean = target <= threshold ? leftMean : rightMean;\n            return sum + (target - mean) ** 2;\n          }, 0);\n          \n          const gain = -variance;\n          if (gain > bestSplit.gain) {\n            bestSplit = { feature: featureIdx, threshold, gain };\n          }\n        }\n      }\n      \n      if (bestSplit.gain === Number.NEGATIVE_INFINITY) {\n        const avg = indices.reduce((sum, i) => sum + targets[i], 0) / indices.length;\n        return { value: avg, isLeaf: true };\n      }\n      \n      const leftIndices = indices.filter(idx => features[idx][bestSplit.feature] <= bestSplit.threshold);\n      const rightIndices = indices.filter(idx => features[idx][bestSplit.feature] > bestSplit.threshold);\n      \n      return {\n        feature: bestSplit.feature,\n        threshold: bestSplit.threshold,\n        left: buildTree(leftIndices, depth + 1),\n        right: buildTree(rightIndices, depth + 1),\n        isLeaf: false\n      };\n    };\n    \n    const tree = buildTree(Array.from({ length: features.length }, (_, i) => i), 0);\n    \n    const predict = (feature: number[]): number => {\n      let node = tree;\n      while (!node.isLeaf) {\n        node = feature[node.feature] <= node.threshold ? node.left : node.right;\n      }\n      return node.value;\n    };\n    \n    const predictions = features.map(predict);\n    const mse = targets.reduce((sum, target, i) => sum + (target - predictions[i]) ** 2, 0) / targets.length;\n    \n    return { tree, predict, loss: Math.sqrt(mse) };\n  }\n\n  // Random Forest (simplified ensemble)\n  private randomForest(features: number[][], targets: number[], params: any) {\n    const { nTrees = 10, maxDepth = 5 } = params;\n    const trees: any[] = [];\n    \n    for (let i = 0; i < nTrees; i++) {\n      // Bootstrap sampling\n      const sampleIndices = Array.from({ length: features.length }, () => \n        Math.floor(Math.random() * features.length)\n      );\n      const sampleFeatures = sampleIndices.map(idx => features[idx]);\n      const sampleTargets = sampleIndices.map(idx => targets[idx]);\n      \n      const tree = this.decisionTree(sampleFeatures, sampleTargets, { maxDepth });\n      trees.push(tree);\n    }\n    \n    const predict = (feature: number[]): number => {\n      const predictions = trees.map(tree => tree.predict(feature));\n      return predictions.reduce((sum, pred) => sum + pred, 0) / predictions.length;\n    };\n    \n    const predictions = features.map(predict);\n    const mse = targets.reduce((sum, target, i) => sum + (target - predictions[i]) ** 2, 0) / targets.length;\n    \n    return { trees, predict, loss: Math.sqrt(mse) };\n  }\n\n  // Neural Network (enhanced from previous)\n  private neuralNetwork(features: number[][], targets: number[], params: any) {\n    const { hiddenSize = 10, learningRate = 0.01, epochs = 500 } = params;\n    const inputSize = features[0].length;\n    \n    // Initialize weights\n    let weightsIH = Array.from({ length: inputSize }, () => \n      Array.from({ length: hiddenSize }, () => Math.random() * 2 - 1)\n    );\n    let weightsHO = Array.from({ length: hiddenSize }, () => Math.random() * 2 - 1);\n    let biasH = Array.from({ length: hiddenSize }, () => Math.random() * 2 - 1);\n    let biasO = Math.random() * 2 - 1;\n    \n    const sigmoid = (x: number) => 1 / (1 + Math.exp(-x));\n    const sigmoidDerivative = (x: number) => x * (1 - x);\n    \n    for (let epoch = 0; epoch < epochs; epoch++) {\n      for (let i = 0; i < features.length; i++) {\n        // Forward pass\n        const hiddenInputs = Array.from({ length: hiddenSize }, (_, h) => \n          features[i].reduce((sum, input, j) => sum + input * weightsIH[j][h], biasH[h])\n        );\n        const hiddenOutputs = hiddenInputs.map(sigmoid);\n        \n        const finalInput = hiddenOutputs.reduce((sum, h, j) => sum + h * weightsHO[j], biasO);\n        const finalOutput = sigmoid(finalInput);\n        \n        // Backward pass\n        const outputError = targets[i] - finalOutput;\n        const outputDelta = outputError * sigmoidDerivative(finalOutput);\n        \n        const hiddenErrors = weightsHO.map(w => outputDelta * w);\n        const hiddenDeltas = hiddenOutputs.map((h, j) => hiddenErrors[j] * sigmoidDerivative(h));\n        \n        // Update weights\n        weightsHO = weightsHO.map((w, j) => w + learningRate * outputDelta * hiddenOutputs[j]);\n        biasO += learningRate * outputDelta;\n        \n        weightsIH = weightsIH.map((row, j) => \n          row.map((w, h) => w + learningRate * hiddenDeltas[h] * features[i][j])\n        );\n        biasH = biasH.map((b, h) => b + learningRate * hiddenDeltas[h]);\n      }\n    }\n    \n    const predict = (feature: number[]): number => {\n      const hiddenInputs = Array.from({ length: hiddenSize }, (_, h) => \n        feature.reduce((sum, input, j) => sum + input * weightsIH[j][h], biasH[h])\n      );\n      const hiddenOutputs = hiddenInputs.map(sigmoid);\n      const finalInput = hiddenOutputs.reduce((sum, h, j) => sum + h * weightsHO[j], biasO);\n      return sigmoid(finalInput);\n    };\n    \n    const predictions = features.map(predict);\n    const mse = targets.reduce((sum, target, i) => sum + (target - predictions[i]) ** 2, 0) / targets.length;\n    \n    return { predict, loss: Math.sqrt(mse) };\n  }\n\n  async autoOptimize(features: number[][], targets: number[], task = 'regression'): Promise<AutoMLResult> {\n    const startTime = Date.now();\n    \n    try {\n      const algorithms = [\n        { name: 'linear', fn: this.linearRegression.bind(this) },\n        { name: 'tree', fn: this.decisionTree.bind(this) },\n        { name: 'ensemble', fn: this.randomForest.bind(this) },\n        { name: 'neural', fn: this.neuralNetwork.bind(this) }\n      ];\n      \n      const hyperparamGrids = {\n        linear: [\n          { learningRate: 0.001, iterations: 500 },\n          { learningRate: 0.01, iterations: 1000 },\n          { learningRate: 0.1, iterations: 2000 }\n        ],\n        tree: [\n          { maxDepth: 3, minSamplesLeaf: 5 },\n          { maxDepth: 5, minSamplesLeaf: 2 },\n          { maxDepth: 10, minSamplesLeaf: 1 }\n        ],\n        ensemble: [\n          { nTrees: 5, maxDepth: 3 },\n          { nTrees: 10, maxDepth: 5 },\n          { nTrees: 20, maxDepth: 7 }\n        ],\n        neural: [\n          { hiddenSize: 5, learningRate: 0.01, epochs: 300 },\n          { hiddenSize: 10, learningRate: 0.05, epochs: 500 },\n          { hiddenSize: 20, learningRate: 0.1, epochs: 800 }\n        ]\n      };\n      \n      const results: ModelConfig[] = [];\n      \n      for (const algorithm of algorithms) {\n        const grid = hyperparamGrids[algorithm.name as keyof typeof hyperparamGrids];\n        \n        for (const params of grid) {\n          const modelStart = Date.now();\n          \n          try {\n            const model = algorithm.fn(features, targets, params);\n            const performance = 1 / (1 + model.loss); // Convert loss to performance score\n            const trainTime = Date.now() - modelStart;\n            \n            const config: ModelConfig = {\n              algorithm: algorithm.name as any,\n              hyperparams: params,\n              performance,\n              trainTime\n            };\n            \n            results.push(config);\n            \n            logger.info(`AutoML model trained: ${algorithm.name}`, {\n              params,\n              performance,\n              trainTime\n            });\n            \n          } catch (error) {\n            logger.warn(`AutoML model failed: ${algorithm.name}`, { params, error });\n          }\n        }\n      }\n      \n      // Find best model\n      const bestModel = results.reduce((best, current) => \n        current.performance > best.performance ? current : best\n      );\n      \n      const improvementPercent = this.bestPerformance > 0 \n        ? ((bestModel.performance - this.bestPerformance) / this.bestPerformance) * 100 \n        : 0;\n      \n      this.bestPerformance = Math.max(this.bestPerformance, bestModel.performance);\n      this.modelHistory.push(bestModel);\n      \n      // Calculate confidence based on performance consistency\n      const topModels = results.sort((a, b) => b.performance - a.performance).slice(0, 3);\n      const avgTopPerformance = topModels.reduce((sum, m) => sum + m.performance, 0) / topModels.length;\n      const confidence = (avgTopPerformance / bestModel.performance) * 100;\n      \n      const totalTime = Date.now() - startTime;\n      \n      logger.info('AutoML optimization complete', {\n        bestAlgorithm: bestModel.algorithm,\n        bestPerformance: bestModel.performance,\n        improvementPercent,\n        totalTime,\n        modelsEvaluated: results.length\n      });\n      \n      return {\n        bestModel,\n        allModels: results.sort((a, b) => b.performance - a.performance),\n        improvementPercent,\n        confidence\n      };\n      \n    } catch (error) {\n      logger.error('AutoML optimization failed', error);\n      throw error;\n    }\n  }\n\n  // Get model performance history\n  getModelHistory(): ModelConfig[] {\n    return [...this.modelHistory];\n  }\n\n  // Check if model needs retraining based on performance drift\n  shouldRetrain(currentPerformance: number, threshold = 0.05): boolean {\n    if (this.bestPerformance === 0) return true;\n    const drift = (this.bestPerformance - currentPerformance) / this.bestPerformance;\n    return drift > threshold;\n  }\n}\n\n// Export singleton AutoML engine\nexport const supremeAutoML = new AutoMLEngine(); "],"names":["AutoMLEngine","supremeAutoML","linearRegression","features","targets","params","learningRate","iterations","m","length","n","weights","Array","fill","bias","totalLoss","iter","weightGradients","biasGradient","i","prediction","reduce","sum","feature","j","error","loss","Math","sqrt","decisionTree","maxDepth","minSamplesLeaf","buildTree","indices","depth","avg","value","isLeaf","bestSplit","threshold","gain","Number","NEGATIVE_INFINITY","featureIdx","values","map","sort","a","b","leftIndices","filter","idx","rightIndices","leftMean","rightMean","variance","target","mean","left","right","tree","from","_","predict","node","predictions","mse","randomForest","nTrees","trees","sampleIndices","floor","random","sampleFeatures","sampleTargets","push","pred","neuralNetwork","hiddenSize","epochs","inputSize","weightsIH","weightsHO","biasH","biasO","sigmoid","x","exp","sigmoidDerivative","epoch","hiddenInputs","h","input","hiddenOutputs","finalInput","finalOutput","outputError","outputDelta","hiddenErrors","w","hiddenDeltas","row","autoOptimize","task","startTime","Date","now","algorithms","name","fn","bind","hyperparamGrids","linear","ensemble","neural","results","algorithm","grid","modelStart","model","performance","trainTime","config","hyperparams","logger","info","warn","bestModel","best","current","improvementPercent","bestPerformance","max","modelHistory","topModels","slice","avgTopPerformance","confidence","totalTime","bestAlgorithm","modelsEvaluated","allModels","getModelHistory","shouldRetrain","currentPerformance","drift"],"mappings":"AAAA;;;;;;;;;;;CAWC;;;;;;;;;;;IAkBYA,YAAY;eAAZA;;IAgUAC,aAAa;eAAbA;;;wBAhVU;AAgBhB,MAAMD;IAIX,+BAA+B;IACvBE,iBAAiBC,QAAoB,EAAEC,OAAiB,EAAEC,MAAW,EAAE;QAC7E,MAAM,EAAEC,eAAe,IAAI,EAAEC,aAAa,IAAI,EAAE,GAAGF;QAEnD,MAAMG,IAAIL,SAASM,MAAM;QACzB,MAAMC,IAAIP,QAAQ,CAAC,EAAE,CAACM,MAAM;QAC5B,MAAME,UAAUC,MAAMF,GAAGG,IAAI,CAAC;QAC9B,IAAIC,OAAO;QACX,IAAIC,YAAY;QAEhB,IAAK,IAAIC,OAAO,GAAGA,OAAOT,YAAYS,OAAQ;YAC5C,MAAMC,kBAAkBL,MAAMF,GAAGG,IAAI,CAAC;YACtC,IAAIK,eAAe;YACnBH,YAAY;YAEZ,IAAK,IAAII,IAAI,GAAGA,IAAIX,GAAGW,IAAK;gBAC1B,MAAMC,aAAajB,QAAQ,CAACgB,EAAE,CAACE,MAAM,CAAC,CAACC,KAAKC,SAASC,IAAMF,MAAMC,UAAUZ,OAAO,CAACa,EAAE,EAAEV;gBACvF,MAAMW,QAAQL,aAAahB,OAAO,CAACe,EAAE;gBACrCJ,aAAaU,QAAQA;gBAErB,IAAK,IAAID,IAAI,GAAGA,IAAId,GAAGc,IAAK;oBAC1BP,eAAe,CAACO,EAAE,IAAI,AAAC,IAAIhB,IAAKiB,QAAQtB,QAAQ,CAACgB,EAAE,CAACK,EAAE;gBACxD;gBACAN,gBAAgB,AAAC,IAAIV,IAAKiB;YAC5B;YAEA,IAAK,IAAID,IAAI,GAAGA,IAAId,GAAGc,IAAK;gBAC1Bb,OAAO,CAACa,EAAE,IAAIlB,eAAeW,eAAe,CAACO,EAAE;YACjD;YACAV,QAAQR,eAAeY;QACzB;QAEA,OAAO;YAAEP;YAASG;YAAMY,MAAMC,KAAKC,IAAI,CAACb,YAAYP;QAAG;IACzD;IAEA,6BAA6B;IACrBqB,aAAa1B,QAAoB,EAAEC,OAAiB,EAAEC,MAAW,EAAE;QACzE,MAAM,EAAEyB,WAAW,CAAC,EAAEC,iBAAiB,CAAC,EAAE,GAAG1B;QAE7C,MAAM2B,YAAY,CAACC,SAAmBC;YACpC,IAAIA,SAASJ,YAAYG,QAAQxB,MAAM,IAAIsB,gBAAgB;gBACzD,MAAMI,MAAMF,QAAQZ,MAAM,CAAC,CAACC,KAAKH,IAAMG,MAAMlB,OAAO,CAACe,EAAE,EAAE,KAAKc,QAAQxB,MAAM;gBAC5E,OAAO;oBAAE2B,OAAOD;oBAAKE,QAAQ;gBAAK;YACpC;YAEA,IAAIC,YAAY;gBAAEf,SAAS;gBAAGgB,WAAW;gBAAGC,MAAMC,OAAOC,iBAAiB;YAAC;YAE3E,IAAK,IAAIC,aAAa,GAAGA,aAAaxC,QAAQ,CAAC,EAAE,CAACM,MAAM,EAAEkC,aAAc;gBACtE,MAAMC,SAASX,QAAQY,GAAG,CAAC1B,CAAAA,IAAKhB,QAAQ,CAACgB,EAAE,CAACwB,WAAW,EAAEG,IAAI,CAAC,CAACC,GAAGC,IAAMD,IAAIC;gBAE5E,IAAK,IAAI7B,IAAI,GAAGA,IAAIyB,OAAOnC,MAAM,EAAEU,IAAK;oBACtC,MAAMoB,YAAY,AAACK,CAAAA,MAAM,CAACzB,EAAE,GAAGyB,MAAM,CAACzB,IAAE,EAAE,AAAD,IAAK;oBAC9C,MAAM8B,cAAchB,QAAQiB,MAAM,CAACC,CAAAA,MAAOhD,QAAQ,CAACgD,IAAI,CAACR,WAAW,IAAIJ;oBACvE,MAAMa,eAAenB,QAAQiB,MAAM,CAACC,CAAAA,MAAOhD,QAAQ,CAACgD,IAAI,CAACR,WAAW,GAAGJ;oBAEvE,IAAIU,YAAYxC,MAAM,KAAK,KAAK2C,aAAa3C,MAAM,KAAK,GAAG;oBAE3D,MAAM4C,WAAWJ,YAAY5B,MAAM,CAAC,CAACC,KAAK6B,MAAQ7B,MAAMlB,OAAO,CAAC+C,IAAI,EAAE,KAAKF,YAAYxC,MAAM;oBAC7F,MAAM6C,YAAYF,aAAa/B,MAAM,CAAC,CAACC,KAAK6B,MAAQ7B,MAAMlB,OAAO,CAAC+C,IAAI,EAAE,KAAKC,aAAa3C,MAAM;oBAEhG,MAAM8C,WAAWtB,QAAQZ,MAAM,CAAC,CAACC,KAAK6B;wBACpC,MAAMK,SAASpD,OAAO,CAAC+C,IAAI;wBAC3B,MAAMM,OAAOD,UAAUjB,YAAYc,WAAWC;wBAC9C,OAAOhC,MAAM,AAACkC,CAAAA,SAASC,IAAG,KAAM;oBAClC,GAAG;oBAEH,MAAMjB,OAAO,CAACe;oBACd,IAAIf,OAAOF,UAAUE,IAAI,EAAE;wBACzBF,YAAY;4BAAEf,SAASoB;4BAAYJ;4BAAWC;wBAAK;oBACrD;gBACF;YACF;YAEA,IAAIF,UAAUE,IAAI,KAAKC,OAAOC,iBAAiB,EAAE;gBAC/C,MAAMP,MAAMF,QAAQZ,MAAM,CAAC,CAACC,KAAKH,IAAMG,MAAMlB,OAAO,CAACe,EAAE,EAAE,KAAKc,QAAQxB,MAAM;gBAC5E,OAAO;oBAAE2B,OAAOD;oBAAKE,QAAQ;gBAAK;YACpC;YAEA,MAAMY,cAAchB,QAAQiB,MAAM,CAACC,CAAAA,MAAOhD,QAAQ,CAACgD,IAAI,CAACb,UAAUf,OAAO,CAAC,IAAIe,UAAUC,SAAS;YACjG,MAAMa,eAAenB,QAAQiB,MAAM,CAACC,CAAAA,MAAOhD,QAAQ,CAACgD,IAAI,CAACb,UAAUf,OAAO,CAAC,GAAGe,UAAUC,SAAS;YAEjG,OAAO;gBACLhB,SAASe,UAAUf,OAAO;gBAC1BgB,WAAWD,UAAUC,SAAS;gBAC9BmB,MAAM1B,UAAUiB,aAAaf,QAAQ;gBACrCyB,OAAO3B,UAAUoB,cAAclB,QAAQ;gBACvCG,QAAQ;YACV;QACF;QAEA,MAAMuB,OAAO5B,UAAUpB,MAAMiD,IAAI,CAAC;YAAEpD,QAAQN,SAASM,MAAM;QAAC,GAAG,CAACqD,GAAG3C,IAAMA,IAAI;QAE7E,MAAM4C,UAAU,CAACxC;YACf,IAAIyC,OAAOJ;YACX,MAAO,CAACI,KAAK3B,MAAM,CAAE;gBACnB2B,OAAOzC,OAAO,CAACyC,KAAKzC,OAAO,CAAC,IAAIyC,KAAKzB,SAAS,GAAGyB,KAAKN,IAAI,GAAGM,KAAKL,KAAK;YACzE;YACA,OAAOK,KAAK5B,KAAK;QACnB;QAEA,MAAM6B,cAAc9D,SAAS0C,GAAG,CAACkB;QACjC,MAAMG,MAAM9D,QAAQiB,MAAM,CAAC,CAACC,KAAKkC,QAAQrC,IAAMG,MAAM,AAACkC,CAAAA,SAASS,WAAW,CAAC9C,EAAE,AAAD,KAAM,GAAG,KAAKf,QAAQK,MAAM;QAExG,OAAO;YAAEmD;YAAMG;YAASrC,MAAMC,KAAKC,IAAI,CAACsC;QAAK;IAC/C;IAEA,sCAAsC;IAC9BC,aAAahE,QAAoB,EAAEC,OAAiB,EAAEC,MAAW,EAAE;QACzE,MAAM,EAAE+D,SAAS,EAAE,EAAEtC,WAAW,CAAC,EAAE,GAAGzB;QACtC,MAAMgE,QAAe,EAAE;QAEvB,IAAK,IAAIlD,IAAI,GAAGA,IAAIiD,QAAQjD,IAAK;YAC/B,qBAAqB;YACrB,MAAMmD,gBAAgB1D,MAAMiD,IAAI,CAAC;gBAAEpD,QAAQN,SAASM,MAAM;YAAC,GAAG,IAC5DkB,KAAK4C,KAAK,CAAC5C,KAAK6C,MAAM,KAAKrE,SAASM,MAAM;YAE5C,MAAMgE,iBAAiBH,cAAczB,GAAG,CAACM,CAAAA,MAAOhD,QAAQ,CAACgD,IAAI;YAC7D,MAAMuB,gBAAgBJ,cAAczB,GAAG,CAACM,CAAAA,MAAO/C,OAAO,CAAC+C,IAAI;YAE3D,MAAMS,OAAO,IAAI,CAAC/B,YAAY,CAAC4C,gBAAgBC,eAAe;gBAAE5C;YAAS;YACzEuC,MAAMM,IAAI,CAACf;QACb;QAEA,MAAMG,UAAU,CAACxC;YACf,MAAM0C,cAAcI,MAAMxB,GAAG,CAACe,CAAAA,OAAQA,KAAKG,OAAO,CAACxC;YACnD,OAAO0C,YAAY5C,MAAM,CAAC,CAACC,KAAKsD,OAAStD,MAAMsD,MAAM,KAAKX,YAAYxD,MAAM;QAC9E;QAEA,MAAMwD,cAAc9D,SAAS0C,GAAG,CAACkB;QACjC,MAAMG,MAAM9D,QAAQiB,MAAM,CAAC,CAACC,KAAKkC,QAAQrC,IAAMG,MAAM,AAACkC,CAAAA,SAASS,WAAW,CAAC9C,EAAE,AAAD,KAAM,GAAG,KAAKf,QAAQK,MAAM;QAExG,OAAO;YAAE4D;YAAON;YAASrC,MAAMC,KAAKC,IAAI,CAACsC;QAAK;IAChD;IAEA,0CAA0C;IAClCW,cAAc1E,QAAoB,EAAEC,OAAiB,EAAEC,MAAW,EAAE;QAC1E,MAAM,EAAEyE,aAAa,EAAE,EAAExE,eAAe,IAAI,EAAEyE,SAAS,GAAG,EAAE,GAAG1E;QAC/D,MAAM2E,YAAY7E,QAAQ,CAAC,EAAE,CAACM,MAAM;QAEpC,qBAAqB;QACrB,IAAIwE,YAAYrE,MAAMiD,IAAI,CAAC;YAAEpD,QAAQuE;QAAU,GAAG,IAChDpE,MAAMiD,IAAI,CAAC;gBAAEpD,QAAQqE;YAAW,GAAG,IAAMnD,KAAK6C,MAAM,KAAK,IAAI;QAE/D,IAAIU,YAAYtE,MAAMiD,IAAI,CAAC;YAAEpD,QAAQqE;QAAW,GAAG,IAAMnD,KAAK6C,MAAM,KAAK,IAAI;QAC7E,IAAIW,QAAQvE,MAAMiD,IAAI,CAAC;YAAEpD,QAAQqE;QAAW,GAAG,IAAMnD,KAAK6C,MAAM,KAAK,IAAI;QACzE,IAAIY,QAAQzD,KAAK6C,MAAM,KAAK,IAAI;QAEhC,MAAMa,UAAU,CAACC,IAAc,IAAK,CAAA,IAAI3D,KAAK4D,GAAG,CAAC,CAACD,EAAC;QACnD,MAAME,oBAAoB,CAACF,IAAcA,IAAK,CAAA,IAAIA,CAAAA;QAElD,IAAK,IAAIG,QAAQ,GAAGA,QAAQV,QAAQU,QAAS;YAC3C,IAAK,IAAItE,IAAI,GAAGA,IAAIhB,SAASM,MAAM,EAAEU,IAAK;gBACxC,eAAe;gBACf,MAAMuE,eAAe9E,MAAMiD,IAAI,CAAC;oBAAEpD,QAAQqE;gBAAW,GAAG,CAAChB,GAAG6B,IAC1DxF,QAAQ,CAACgB,EAAE,CAACE,MAAM,CAAC,CAACC,KAAKsE,OAAOpE,IAAMF,MAAMsE,QAAQX,SAAS,CAACzD,EAAE,CAACmE,EAAE,EAAER,KAAK,CAACQ,EAAE;gBAE/E,MAAME,gBAAgBH,aAAa7C,GAAG,CAACwC;gBAEvC,MAAMS,aAAaD,cAAcxE,MAAM,CAAC,CAACC,KAAKqE,GAAGnE,IAAMF,MAAMqE,IAAIT,SAAS,CAAC1D,EAAE,EAAE4D;gBAC/E,MAAMW,cAAcV,QAAQS;gBAE5B,gBAAgB;gBAChB,MAAME,cAAc5F,OAAO,CAACe,EAAE,GAAG4E;gBACjC,MAAME,cAAcD,cAAcR,kBAAkBO;gBAEpD,MAAMG,eAAehB,UAAUrC,GAAG,CAACsD,CAAAA,IAAKF,cAAcE;gBACtD,MAAMC,eAAeP,cAAchD,GAAG,CAAC,CAAC8C,GAAGnE,IAAM0E,YAAY,CAAC1E,EAAE,GAAGgE,kBAAkBG;gBAErF,iBAAiB;gBACjBT,YAAYA,UAAUrC,GAAG,CAAC,CAACsD,GAAG3E,IAAM2E,IAAI7F,eAAe2F,cAAcJ,aAAa,CAACrE,EAAE;gBACrF4D,SAAS9E,eAAe2F;gBAExBhB,YAAYA,UAAUpC,GAAG,CAAC,CAACwD,KAAK7E,IAC9B6E,IAAIxD,GAAG,CAAC,CAACsD,GAAGR,IAAMQ,IAAI7F,eAAe8F,YAAY,CAACT,EAAE,GAAGxF,QAAQ,CAACgB,EAAE,CAACK,EAAE;gBAEvE2D,QAAQA,MAAMtC,GAAG,CAAC,CAACG,GAAG2C,IAAM3C,IAAI1C,eAAe8F,YAAY,CAACT,EAAE;YAChE;QACF;QAEA,MAAM5B,UAAU,CAACxC;YACf,MAAMmE,eAAe9E,MAAMiD,IAAI,CAAC;gBAAEpD,QAAQqE;YAAW,GAAG,CAAChB,GAAG6B,IAC1DpE,QAAQF,MAAM,CAAC,CAACC,KAAKsE,OAAOpE,IAAMF,MAAMsE,QAAQX,SAAS,CAACzD,EAAE,CAACmE,EAAE,EAAER,KAAK,CAACQ,EAAE;YAE3E,MAAME,gBAAgBH,aAAa7C,GAAG,CAACwC;YACvC,MAAMS,aAAaD,cAAcxE,MAAM,CAAC,CAACC,KAAKqE,GAAGnE,IAAMF,MAAMqE,IAAIT,SAAS,CAAC1D,EAAE,EAAE4D;YAC/E,OAAOC,QAAQS;QACjB;QAEA,MAAM7B,cAAc9D,SAAS0C,GAAG,CAACkB;QACjC,MAAMG,MAAM9D,QAAQiB,MAAM,CAAC,CAACC,KAAKkC,QAAQrC,IAAMG,MAAM,AAACkC,CAAAA,SAASS,WAAW,CAAC9C,EAAE,AAAD,KAAM,GAAG,KAAKf,QAAQK,MAAM;QAExG,OAAO;YAAEsD;YAASrC,MAAMC,KAAKC,IAAI,CAACsC;QAAK;IACzC;IAEA,MAAMoC,aAAanG,QAAoB,EAAEC,OAAiB,EAAEmG,OAAO,YAAY,EAAyB;QACtG,MAAMC,YAAYC,KAAKC,GAAG;QAE1B,IAAI;YACF,MAAMC,aAAa;gBACjB;oBAAEC,MAAM;oBAAUC,IAAI,IAAI,CAAC3G,gBAAgB,CAAC4G,IAAI,CAAC,IAAI;gBAAE;gBACvD;oBAAEF,MAAM;oBAAQC,IAAI,IAAI,CAAChF,YAAY,CAACiF,IAAI,CAAC,IAAI;gBAAE;gBACjD;oBAAEF,MAAM;oBAAYC,IAAI,IAAI,CAAC1C,YAAY,CAAC2C,IAAI,CAAC,IAAI;gBAAE;gBACrD;oBAAEF,MAAM;oBAAUC,IAAI,IAAI,CAAChC,aAAa,CAACiC,IAAI,CAAC,IAAI;gBAAE;aACrD;YAED,MAAMC,kBAAkB;gBACtBC,QAAQ;oBACN;wBAAE1G,cAAc;wBAAOC,YAAY;oBAAI;oBACvC;wBAAED,cAAc;wBAAMC,YAAY;oBAAK;oBACvC;wBAAED,cAAc;wBAAKC,YAAY;oBAAK;iBACvC;gBACDqD,MAAM;oBACJ;wBAAE9B,UAAU;wBAAGC,gBAAgB;oBAAE;oBACjC;wBAAED,UAAU;wBAAGC,gBAAgB;oBAAE;oBACjC;wBAAED,UAAU;wBAAIC,gBAAgB;oBAAE;iBACnC;gBACDkF,UAAU;oBACR;wBAAE7C,QAAQ;wBAAGtC,UAAU;oBAAE;oBACzB;wBAAEsC,QAAQ;wBAAItC,UAAU;oBAAE;oBAC1B;wBAAEsC,QAAQ;wBAAItC,UAAU;oBAAE;iBAC3B;gBACDoF,QAAQ;oBACN;wBAAEpC,YAAY;wBAAGxE,cAAc;wBAAMyE,QAAQ;oBAAI;oBACjD;wBAAED,YAAY;wBAAIxE,cAAc;wBAAMyE,QAAQ;oBAAI;oBAClD;wBAAED,YAAY;wBAAIxE,cAAc;wBAAKyE,QAAQ;oBAAI;iBAClD;YACH;YAEA,MAAMoC,UAAyB,EAAE;YAEjC,KAAK,MAAMC,aAAaT,WAAY;gBAClC,MAAMU,OAAON,eAAe,CAACK,UAAUR,IAAI,CAAiC;gBAE5E,KAAK,MAAMvG,UAAUgH,KAAM;oBACzB,MAAMC,aAAab,KAAKC,GAAG;oBAE3B,IAAI;wBACF,MAAMa,QAAQH,UAAUP,EAAE,CAAC1G,UAAUC,SAASC;wBAC9C,MAAMmH,cAAc,IAAK,CAAA,IAAID,MAAM7F,IAAI,AAAD,GAAI,oCAAoC;wBAC9E,MAAM+F,YAAYhB,KAAKC,GAAG,KAAKY;wBAE/B,MAAMI,SAAsB;4BAC1BN,WAAWA,UAAUR,IAAI;4BACzBe,aAAatH;4BACbmH;4BACAC;wBACF;wBAEAN,QAAQxC,IAAI,CAAC+C;wBAEbE,cAAM,CAACC,IAAI,CAAC,CAAC,sBAAsB,EAAET,UAAUR,IAAI,EAAE,EAAE;4BACrDvG;4BACAmH;4BACAC;wBACF;oBAEF,EAAE,OAAOhG,OAAO;wBACdmG,cAAM,CAACE,IAAI,CAAC,CAAC,qBAAqB,EAAEV,UAAUR,IAAI,EAAE,EAAE;4BAAEvG;4BAAQoB;wBAAM;oBACxE;gBACF;YACF;YAEA,kBAAkB;YAClB,MAAMsG,YAAYZ,QAAQ9F,MAAM,CAAC,CAAC2G,MAAMC,UACtCA,QAAQT,WAAW,GAAGQ,KAAKR,WAAW,GAAGS,UAAUD;YAGrD,MAAME,qBAAqB,IAAI,CAACC,eAAe,GAAG,IAC9C,AAAEJ,CAAAA,UAAUP,WAAW,GAAG,IAAI,CAACW,eAAe,AAAD,IAAK,IAAI,CAACA,eAAe,GAAI,MAC1E;YAEJ,IAAI,CAACA,eAAe,GAAGxG,KAAKyG,GAAG,CAAC,IAAI,CAACD,eAAe,EAAEJ,UAAUP,WAAW;YAC3E,IAAI,CAACa,YAAY,CAAC1D,IAAI,CAACoD;YAEvB,wDAAwD;YACxD,MAAMO,YAAYnB,QAAQrE,IAAI,CAAC,CAACC,GAAGC,IAAMA,EAAEwE,WAAW,GAAGzE,EAAEyE,WAAW,EAAEe,KAAK,CAAC,GAAG;YACjF,MAAMC,oBAAoBF,UAAUjH,MAAM,CAAC,CAACC,KAAKd,IAAMc,MAAMd,EAAEgH,WAAW,EAAE,KAAKc,UAAU7H,MAAM;YACjG,MAAMgI,aAAa,AAACD,oBAAoBT,UAAUP,WAAW,GAAI;YAEjE,MAAMkB,YAAYjC,KAAKC,GAAG,KAAKF;YAE/BoB,cAAM,CAACC,IAAI,CAAC,gCAAgC;gBAC1Cc,eAAeZ,UAAUX,SAAS;gBAClCe,iBAAiBJ,UAAUP,WAAW;gBACtCU;gBACAQ;gBACAE,iBAAiBzB,QAAQ1G,MAAM;YACjC;YAEA,OAAO;gBACLsH;gBACAc,WAAW1B,QAAQrE,IAAI,CAAC,CAACC,GAAGC,IAAMA,EAAEwE,WAAW,GAAGzE,EAAEyE,WAAW;gBAC/DU;gBACAO;YACF;QAEF,EAAE,OAAOhH,OAAO;YACdmG,cAAM,CAACnG,KAAK,CAAC,8BAA8BA;YAC3C,MAAMA;QACR;IACF;IAEA,gCAAgC;IAChCqH,kBAAiC;QAC/B,OAAO;eAAI,IAAI,CAACT,YAAY;SAAC;IAC/B;IAEA,6DAA6D;IAC7DU,cAAcC,kBAA0B,EAAEzG,YAAY,IAAI,EAAW;QACnE,IAAI,IAAI,CAAC4F,eAAe,KAAK,GAAG,OAAO;QACvC,MAAMc,QAAQ,AAAC,CAAA,IAAI,CAACd,eAAe,GAAGa,kBAAiB,IAAK,IAAI,CAACb,eAAe;QAChF,OAAOc,QAAQ1G;IACjB;;aA3TQ8F,eAA8B,EAAE;aAChCF,kBAAkB;;AA2T5B;AAGO,MAAMlI,gBAAgB,IAAID"}