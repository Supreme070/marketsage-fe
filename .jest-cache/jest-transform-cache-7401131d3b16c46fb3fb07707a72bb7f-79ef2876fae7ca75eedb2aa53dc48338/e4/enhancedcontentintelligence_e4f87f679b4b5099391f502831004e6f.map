{"version":3,"sources":["/Users/supreme/Desktop/marketsage/src/lib/ai/enhanced-content-intelligence.ts"],"sourcesContent":["/**\n * Enhanced Content Intelligence Module\n * Advanced NLP and ML-based content analysis and optimization\n */\n\nimport natural from 'natural';\nimport sentiment from 'sentiment';\nimport nlp from 'compromise';\nimport { logger, LogContext } from '@/lib/logger';\nimport prisma from '@/lib/db/prisma';\nimport { type ContentType, SentimentAnalysisResult, type ContentScoreResult } from '@/lib/content-intelligence';\nimport { PorterStemmer, SentimentAnalyzer as NaturalSentiment, LogisticRegressionClassifier } from 'natural';\nimport { distance as levenshtein } from 'natural/lib/natural/distance/levenshtein_distance';\nimport { NGrams, WordTokenizer, BayesClassifier } from 'natural';\nimport type { TfIdf as NaturalTfIdf } from 'natural';\n\n// Initialize NLP tools\nconst tokenizer = new natural.WordTokenizer();\nconst tfidf = new natural.TfIdf();\nconst sentimentAnalyzer = new sentiment();\n\n// Advanced ML Models Configuration\nconst BERT_CONFIG = {\n  modelPath: './models/bert-base-uncased',\n  maxLength: 512,\n  batchSize: 32\n};\n\n// Add new interfaces for classification and advanced sentiment\ninterface TextClassificationResult {\n  category: string;\n  confidence: number;\n  subcategories: Array<{\n    name: string;\n    confidence: number;\n  }>;\n  topics: Array<{\n    name: string;\n    relevance: number;\n  }>;\n}\n\ninterface AdvancedSentimentResult {\n  overall: {\n    score: number;\n    label: 'positive' | 'negative' | 'neutral';\n    confidence: number;\n  };\n  aspects: Array<{\n    aspect: string;\n    sentiment: {\n      score: number;\n      label: 'positive' | 'negative' | 'neutral';\n    };\n    examples: string[];\n  }>;\n  emotions: {\n    joy: number;\n    sadness: number;\n    anger: number;\n    fear: number;\n    surprise: number;\n    trust: number;\n    anticipation: number;\n    disgust: number;\n  };\n  /**\n   * Fine-grained emotion scores produced by the classifier.\n   * The keys correspond to every label present in EMOTION_TRAINING_DATA.\n   */\n  granularEmotions: Record<string, number>;\n  intensity: number;\n  subjectivity: number;\n  sarcasm: {\n    detected: boolean;\n    confidence: number;\n  };\n  // Legacy raw sentiment fields kept optional for backward compatibility\n  score?: number;\n  comparative?: number;\n  tokens?: string[];\n  positive?: string[];\n  negative?: string[];\n  confidence?: number;\n  aspectBasedSentiment?: any;\n  entitySentiment?: any;\n}\n\ninterface ContentFeatures {\n  tokens: string[];\n  ngrams: string[];\n  keyPhrases: string[];\n  entities: string[];\n  readabilityMetrics: {\n    fleschKincaid: number;\n    gunningFog: number;\n    smog: number;\n    automatedReadability: number;\n  };\n  stylometricFeatures: {\n    avgSentenceLength: number;\n    avgWordLength: number;\n    lexicalDiversity: number;\n    punctuationRatio: number;\n  };\n}\n\ninterface SemanticAnalysisResult {\n  summary: string;\n  keywords: Array<{\n    word: string;\n    score: number;\n    type: 'topic' | 'action' | 'entity';\n  }>;\n  topics: Array<{\n    name: string;\n    confidence: number;\n    relatedTerms: string[];\n  }>;\n  semanticSimilarity: number;\n  coherence: number;\n}\n\ninterface TextSummarizationResult {\n  shortSummary: string;\n  longSummary: string;\n  keyPoints: string[];\n  coverage: number;\n}\n\ninterface KeywordExtractionResult {\n  keywords: Array<{\n    term: string;\n    score: number;\n    frequency: number;\n    position: number[];\n  }>;\n  phrases: Array<{\n    text: string;\n    score: number;\n    words: string[];\n  }>;\n}\n\n// Add missing interfaces and types\ninterface ClassificationScore {\n  label: string;\n  value: number;\n}\n\ninterface Topic {\n  name: string;\n  relevance: number;\n}\n\n// Initialize classifiers\nconst contentClassifier = new LogisticRegressionClassifier();\nconst emotionClassifier = new BayesClassifier();\nconst topicClassifier = new BayesClassifier();\n\n// Training data for classifiers\nconst EMOTION_TRAINING_DATA = {\n  joy: ['happy', 'excited', 'delighted', 'pleased', 'joyful'],\n  sadness: ['sad', 'disappointed', 'unhappy', 'depressed', 'gloomy'],\n  anger: ['angry', 'furious', 'outraged', 'irritated', 'annoyed'],\n  fear: ['scared', 'afraid', 'terrified', 'anxious', 'worried'],\n  surprise: ['surprised', 'amazed', 'astonished', 'shocked', 'stunned'],\n  trust: ['trust', 'reliable', 'dependable', 'confident', 'faithful'],\n  anticipation: ['expect', 'anticipate', 'await', 'looking forward', 'hopeful'],\n  disgust: ['disgusted', 'repulsed', 'revolted', 'appalled', 'horrified'],\n  // Granular sub-emotions\n  happiness: ['happy', 'joyful', 'cheerful', 'gleeful', 'merry'],\n  contentment: ['content', 'satisfied', 'serene', 'fulfilled', 'pleased'],\n  excitement: ['excited', 'thrilled', 'eager', 'enthusiastic', 'pumped'],\n  frustration: ['frustrated', 'annoyed', 'irritated', 'upset', 'exasperated'],\n  anxiety: ['anxious', 'nervous', 'uneasy', 'restless', 'tense'],\n  wonder: ['wonder', 'awe', 'astonishment', 'amazement', 'marvel'],\n  confidence: ['confident', 'assured', 'certain', 'secure', 'positive'],\n  optimism: ['optimistic', 'hopeful', 'encouraged', 'upbeat', 'positive'],\n  contempt: ['contempt', 'scorn', 'disdain', 'revulsion', 'loathing']\n};\n\n// Mapping of granular emotion labels back to the 8 primary categories\nconst GRANULAR_TO_BASE_MAP: Record<string, keyof Omit<AdvancedSentimentResult['emotions'], never>> = {\n  happiness: 'joy',\n  contentment: 'joy',\n  excitement: 'joy',\n  frustration: 'anger',\n  anxiety: 'fear',\n  wonder: 'surprise',\n  confidence: 'trust',\n  optimism: 'anticipation',\n  contempt: 'disgust'\n};\n\n/**\n * Enhanced sentiment analysis using multiple models and aspect-based analysis\n */\nexport async function enhancedSentimentAnalysis(\n  content: string,\n  contentType: ContentType\n): Promise<AdvancedSentimentResult> {\n  try {\n    // Tokenize and preprocess\n    const tokens = tokenizer.tokenize(content.toLowerCase()) || [];\n    const doc = nlp(content);\n    \n    // Get base sentiment using multiple models\n    const vaderResult = sentimentAnalyzer.analyze(content);\n    \n    // Extract named entities\n    const entities = doc.topics().json();\n    \n    // Perform aspect-based sentiment analysis\n    const aspects = await extractAspects(content);\n    const aspectSentiments = await analyzeAspectSentiments(content, aspects);\n    \n    // Entity-level sentiment\n    const entitySentiments = await analyzeEntitySentiments(content, entities);\n    \n    // Calculate confidence based on model agreement\n    const confidence = calculateModelConfidence([\n      vaderResult.score,\n      // Add other model scores here\n    ]);\n    \n    // Combine results\n    return {\n      score: vaderResult.score,\n      comparative: vaderResult.comparative,\n      tokens: tokens,\n      positive: vaderResult.positive || [],\n      negative: vaderResult.negative || [],\n      confidence: confidence,\n      emotions: await analyzeEmotions(content),\n      aspectBasedSentiment: aspectSentiments,\n      entitySentiment: entitySentiments\n    };\n  } catch (error) {\n    const errorContext = {\n      error: error instanceof Error ? error.message : String(error),\n      contentType: contentType as string,\n      contentLength: content.length\n    };\n    logger.error('Error in enhanced sentiment analysis', errorContext);\n    throw new Error('Failed to perform enhanced sentiment analysis');\n  }\n}\n\n/**\n * Extract advanced content features for ML models\n */\nexport async function extractContentFeatures(content: string): Promise<ContentFeatures> {\n  const tokens = tokenizer.tokenize(content) || [];\n  const doc = nlp(content);\n  \n  // Generate n-grams\n  const bigrams = natural.NGrams.bigrams(tokens);\n  const trigrams = natural.NGrams.trigrams(tokens);\n  \n  // Extract key phrases using TF-IDF\n  tfidf.addDocument(content);\n  const keyPhrases = getTopTfIdfTerms(tfidf, 10);\n  \n  // Calculate readability metrics\n  const readabilityMetrics = {\n    fleschKincaid: calculateFleschKincaid(content),\n    gunningFog: calculateGunningFog(content),\n    smog: calculateSMOG(content),\n    automatedReadability: calculateARI(content)\n  };\n\n  // Split content into sentences\n  const sentences = content.split(/[.!?]+/).filter(s => s.trim().length > 0);\n  \n  // Calculate stylometric features\n  const stylometricFeatures = {\n    avgSentenceLength: sentences.reduce((sum, s) => sum + s.split(/\\s+/).length, 0) / sentences.length,\n    avgWordLength: tokens.reduce((sum, w) => sum + w.length, 0) / tokens.length,\n    lexicalDiversity: new Set(tokens.map(t => t.toLowerCase())).size / tokens.length,\n    punctuationRatio: (content.match(/[.,!?;:]/g) || []).length / content.length\n  };\n  \n  const topics = doc.topics().out('array') || [];\n  \n  return {\n    tokens,\n    ngrams: [...bigrams, ...trigrams].map(ng => ng.join(' ')),\n    keyPhrases,\n    entities: topics,\n    readabilityMetrics,\n    stylometricFeatures\n  };\n}\n\n/**\n * Enhanced content scoring using ML models and historical data\n */\nexport async function enhancedContentScoring(\n  content: string,\n  contentType: ContentType\n): Promise<ContentScoreResult> {\n  try {\n    // Extract features\n    const features = await extractContentFeatures(content);\n    \n    // Get historical performance data\n    const historicalData = await getHistoricalPerformance(contentType);\n    \n    // Calculate advanced metrics\n    const readabilityScore = calculateEnhancedReadabilityScore(features);\n    const engagementScore = await predictEngagementScore(features, historicalData);\n    const conversionScore = await predictConversionScore(features, historicalData);\n    const sentimentResult = await enhancedSentimentAnalysis(content, contentType);\n    \n    // Generate ML-based improvements\n    const { improvements, strengths } = await generateMLBasedRecommendations(\n      features,\n      {\n        readabilityScore,\n        engagementScore,\n        conversionScore,\n        sentimentScore: sentimentResult.score\n      }\n    );\n    \n    return {\n      overallScore: calculateOverallScore([\n        readabilityScore,\n        engagementScore,\n        conversionScore,\n        sentimentResult.score\n      ]),\n      readabilityScore,\n      engagementScore,\n      conversionScore,\n      sentimentScore: sentimentResult.score,\n      improvements,\n      strengths\n    };\n  } catch (error) {\n    const errorContext = {\n      error: error instanceof Error ? error.message : String(error),\n      contentType: contentType as string,\n      contentLength: content.length\n    };\n    logger.error('Error in enhanced content scoring', errorContext);\n    throw new Error('Failed to perform enhanced content scoring');\n  }\n}\n\n// Helper functions\n\nasync function extractAspects(content: string): Promise<string[]> {\n  const doc = nlp(content);\n  return doc.nouns().out('array');\n}\n\nasync function analyzeAspectSentiments(content: string, aspects: string[]) {\n  const results: any = {};\n  for (const aspect of aspects) {\n    const relevantSentences = extractRelevantSentences(content, aspect);\n    results[aspect] = {\n      score: await calculateAspectSentiment(relevantSentences),\n      confidence: calculateConfidenceScore(relevantSentences.length),\n      aspects: findRelatedAspects(aspect, aspects)\n    };\n  }\n  return results;\n}\n\nasync function analyzeEntitySentiments(content: string, entities: any[]) {\n  return entities.map(entity => ({\n    entity: entity.text,\n    sentiment: calculateEntitySentiment(content, entity),\n    confidence: calculateConfidenceScore(entity.count)\n  }));\n}\n\nasync function analyzeEmotions(content: string) {\n  const doc = nlp(content);\n  return {\n    joy: calculateEmotionIntensity(doc, 'joy'),\n    sadness: calculateEmotionIntensity(doc, 'sadness'),\n    anger: calculateEmotionIntensity(doc, 'anger'),\n    fear: calculateEmotionIntensity(doc, 'fear'),\n    surprise: calculateEmotionIntensity(doc, 'surprise')\n  };\n}\n\nfunction calculateEmotionIntensity(doc: any, emotion: string): number {\n  // Implement emotion-specific intensity calculation\n  return 0.5; // Placeholder\n}\n\nfunction calculateModelConfidence(scores: number[]): number {\n  const variance = calculateVariance(scores);\n  return 1 - Math.min(variance, 1);\n}\n\nfunction calculateVariance(numbers: number[]): number {\n  const mean = numbers.reduce((a, b) => a + b) / numbers.length;\n  const variance = numbers.reduce((a, b) => a + Math.pow(b - mean, 2), 0) / numbers.length;\n  return Math.sqrt(variance);\n}\n\nasync function getHistoricalPerformance(contentType: ContentType) {\n  return await prisma.contentAnalysis.findMany({\n    where: { contentType },\n    select: {\n      originalContent: true,\n      result: true\n    },\n    orderBy: { createdAt: 'desc' },\n    take: 1000\n  });\n}\n\n/**\n * Calculate Flesch-Kincaid Grade Level\n * Formula: 0.39 * (words/sentences) + 11.8 * (syllables/words) - 15.59\n */\nfunction calculateFleschKincaid(text: string): number {\n  const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);\n  const words = text.split(/\\s+/).filter(w => w.length > 0);\n  const syllables = words.reduce((count, word) => count + countSyllables(word), 0);\n  \n  if (sentences.length === 0 || words.length === 0) return 0;\n  \n  const wordsPerSentence = words.length / sentences.length;\n  const syllablesPerWord = syllables / words.length;\n  \n  return 0.39 * wordsPerSentence + 11.8 * syllablesPerWord - 15.59;\n}\n\n/**\n * Calculate Gunning Fog Index\n * Formula: 0.4 * ((words/sentences) + 100 * (complex words/words))\n */\nfunction calculateGunningFog(text: string): number {\n  const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);\n  const words = text.split(/\\s+/).filter(w => w.length > 0);\n  const complexWords = words.filter(word => countSyllables(word) > 2);\n  \n  if (sentences.length === 0 || words.length === 0) return 0;\n  \n  const wordsPerSentence = words.length / sentences.length;\n  const complexWordPercentage = (complexWords.length / words.length) * 100;\n  \n  return 0.4 * (wordsPerSentence + complexWordPercentage);\n}\n\n/**\n * Calculate SMOG Index\n * Formula: 1.043 * sqrt(30 * complex words / sentences) + 3.1291\n */\nfunction calculateSMOG(text: string): number {\n  const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);\n  const words = text.split(/\\s+/).filter(w => w.length > 0);\n  const complexWords = words.filter(word => countSyllables(word) > 2);\n  \n  if (sentences.length === 0) return 0;\n  \n  return 1.043 * Math.sqrt((30 * complexWords.length) / sentences.length) + 3.1291;\n}\n\n/**\n * Calculate Automated Readability Index\n * Formula: 4.71 * (characters/words) + 0.5 * (words/sentences) - 21.43\n */\nfunction calculateARI(text: string): number {\n  const sentences = text.split(/[.!?]+/).filter(s => s.trim().length > 0);\n  const words = text.split(/\\s+/).filter(w => w.length > 0);\n  const characters = text.replace(/\\s+/g, '').length;\n  \n  if (sentences.length === 0 || words.length === 0) return 0;\n  \n  const charsPerWord = characters / words.length;\n  const wordsPerSentence = words.length / sentences.length;\n  \n  return 4.71 * charsPerWord + 0.5 * wordsPerSentence - 21.43;\n}\n\n/**\n * Count syllables in a word using basic rules\n */\nfunction countSyllables(word: string): number {\n  word = word.toLowerCase().replace(/[^a-z]/g, '');\n  if (word.length <= 3) return 1;\n  \n  // Remove common silent 'e' at the end\n  word = word.replace(/e$/, '');\n  \n  // Count vowel groups\n  const vowelGroups = word.match(/[aeiouy]+/g);\n  return vowelGroups ? vowelGroups.length : 1;\n}\n\n/**\n * Get top TF-IDF terms from a document\n */\nfunction getTopTfIdfTerms(tfidf: natural.TfIdf, n: number): string[] {\n  const terms: Array<{term: string, score: number}> = [];\n  \n  tfidf.listTerms(0).forEach(item => {\n    terms.push({ term: item.term, score: item.tfidf });\n  });\n  \n  return terms\n    .sort((a, b) => b.score - a.score)\n    .slice(0, n)\n    .map(item => item.term);\n}\n\nasync function predictEngagementScore(\n  features: ContentFeatures,\n  historicalData: any[]\n): Promise<number> {\n  // Implement ML-based engagement prediction\n  return 75; // Placeholder\n}\n\nasync function predictConversionScore(\n  features: ContentFeatures,\n  historicalData: any[]\n): Promise<number> {\n  // Implement ML-based conversion prediction\n  return 80; // Placeholder\n}\n\nasync function generateMLBasedRecommendations(\n  features: ContentFeatures,\n  scores: {\n    readabilityScore: number;\n    engagementScore: number;\n    conversionScore: number;\n    sentimentScore: number;\n  }\n): Promise<{ improvements: string[]; strengths: string[] }> {\n  // Implement ML-based recommendation generation\n  return {\n    improvements: [],\n    strengths: []\n  };\n}\n\nfunction calculateOverallScore(scores: number[]): number {\n  return Math.round(scores.reduce((a, b) => a + b) / scores.length);\n}\n\n// Export additional helper functions as needed\nexport const contentIntelligenceHelpers = {\n  extractContentFeatures,\n  analyzeEmotions,\n  calculateModelConfidence\n};\n\n// Add missing helper functions\nfunction extractRelevantSentences(content: string, aspect: string): string[] {\n  return content\n    .split(/[.!?]+/)\n    .filter(sentence => sentence.toLowerCase().includes(aspect.toLowerCase()));\n}\n\nasync function calculateAspectSentiment(sentences: string[]): Promise<number> {\n  if (sentences.length === 0) return 0;\n  \n  const sentiments = sentences.map(s => sentimentAnalyzer.analyze(s).score);\n  return sentiments.reduce((a, b) => a + b, 0) / sentiments.length;\n}\n\nfunction calculateConfidenceScore(count: number): number {\n  return Math.min(1, Math.log(count + 1) / Math.log(10));\n}\n\nfunction findRelatedAspects(aspect: string, allAspects: string[]): string[] {\n  return allAspects.filter(a => \n    a !== aspect && \n    (a.includes(aspect) || aspect.includes(a))\n  );\n}\n\nfunction calculateEntitySentiment(content: string, entity: any): number {\n  const relevantSentences = extractRelevantSentences(content, entity.text);\n  const sentiments = relevantSentences.map(s => sentimentAnalyzer.analyze(s).score);\n  return sentiments.length > 0 \n    ? sentiments.reduce((a, b) => a + b, 0) / sentiments.length \n    : 0;\n}\n\nfunction calculateEnhancedReadabilityScore(features: ContentFeatures): number {\n  const {\n    fleschKincaid,\n    gunningFog,\n    smog,\n    automatedReadability\n  } = features.readabilityMetrics;\n  \n  // Normalize and combine different readability metrics\n  return Math.round(\n    (normalizeScore(fleschKincaid) +\n     normalizeScore(gunningFog) +\n     normalizeScore(smog) +\n     normalizeScore(automatedReadability)) / 4\n  );\n}\n\nfunction normalizeScore(score: number): number {\n  // Normalize score to 0-100 range\n  return Math.max(0, Math.min(100, (100 - score * 10)));\n}\n\n// Add new NLP features\n\n/**\n * Perform semantic analysis of content\n */\nexport async function analyzeSemantics(content: string): Promise<SemanticAnalysisResult> {\n  try {\n    const doc = nlp(content);\n    const tokens = tokenizer.tokenize(content) || [];\n    \n    // Extract topics and entities\n    const topics = await extractTopics(content);\n    const entities = doc.topics().json();\n    \n    // Calculate semantic coherence\n    const coherence = calculateSemanticCoherence(tokens);\n    \n    // Extract keywords with their types\n    const keywordResults = await extractEnhancedKeywords(content);\n    \n    // Calculate semantic similarity with historical content\n    const similarity = await calculateSemanticSimilarity(content);\n    \n    // Generate summary using extractive summarization\n    const summaryResult = await generateSummary(content);\n    \n    return {\n      summary: summaryResult.shortSummary,\n      keywords: keywordResults.keywords.map(k => ({\n        word: k.term,\n        score: k.score,\n        type: determineKeywordType(k.term, entities)\n      })),\n      topics: topics.map(t => ({\n        name: t.topic,\n        confidence: t.confidence,\n        relatedTerms: t.related\n      })),\n      semanticSimilarity: similarity,\n      coherence\n    };\n  } catch (error) {\n    const errorContext = {\n      error: error instanceof Error ? error.message : String(error),\n      contentLength: content.length\n    };\n    logger.error('Error in semantic analysis', errorContext);\n    throw new Error('Failed to perform semantic analysis');\n  }\n}\n\n/**\n * Generate intelligent text summaries\n */\nexport async function generateSummary(content: string): Promise<TextSummarizationResult> {\n  try {\n    const sentences = content.split(/[.!?]+/).filter(s => s.trim().length > 0);\n    \n    // Calculate sentence importance scores\n    const sentenceScores = calculateSentenceImportance(sentences);\n    \n    // Generate summaries of different lengths\n    const shortSummary = await generateExtractiveSummary(sentences, sentenceScores, 0.2);\n    const longSummary = await generateExtractiveSummary(sentences, sentenceScores, 0.4);\n    \n    // Extract key points using topic modeling\n    const keyPoints = await extractKeyPoints(content);\n    \n    // Calculate coverage score\n    const coverage = calculateSummaryCoverage(content, shortSummary);\n    \n    return {\n      shortSummary,\n      longSummary,\n      keyPoints,\n      coverage\n    };\n  } catch (error) {\n    const errorContext = {\n      error: error instanceof Error ? error.message : String(error),\n      contentLength: content.length\n    };\n    logger.error('Error generating summary', errorContext);\n    throw new Error('Failed to generate summary');\n  }\n}\n\n/**\n * Enhanced keyword extraction with phrase mining\n */\nexport async function extractEnhancedKeywords(content: string): Promise<KeywordExtractionResult> {\n  try {\n    const tokens = tokenizer.tokenize(content) || [];\n    const doc = nlp(content);\n    \n    // TF-IDF based keyword extraction\n    const tfidfKeywords = extractTfIdfKeywords(content, tokens);\n    \n    // TextRank based phrase extraction\n    const textRankPhrases = extractTextRankPhrases(content, tokens);\n    \n    // Position-based scoring\n    const positionScores = calculatePositionScores(tokens);\n    \n    // Combine results\n    return {\n      keywords: tfidfKeywords.map(k => ({\n        term: k.term,\n        score: k.score,\n        frequency: k.frequency,\n        position: positionScores[k.term] || []\n      })),\n      phrases: textRankPhrases\n    };\n  } catch (error) {\n    const errorContext = {\n      error: error instanceof Error ? error.message : String(error),\n      contentLength: content.length\n    };\n    logger.error('Error extracting keywords', errorContext);\n    throw new Error('Failed to extract keywords');\n  }\n}\n\n// Helper functions for new NLP features\n\nasync function extractTopics(content: string): Promise<Array<{topic: string; confidence: number; related: string[]}>> {\n  const doc = nlp(content);\n  const topics: Array<{topic: string; confidence: number; related: string[]}> = [];\n  \n  // Extract nouns and noun phrases\n  const nouns = doc.nouns().out('array');\n  const phrases = doc.match('#Noun+ (#Preposition? #Noun+)?').out('array');\n  \n  // Calculate topic scores using TF-IDF\n  const tfidf = new natural.TfIdf();\n  tfidf.addDocument(content);\n  \n  // Process each potential topic\n  const processedTopics = new Set<string>();\n  [...nouns, ...phrases].forEach(topic => {\n    if (!processedTopics.has(topic)) {\n      processedTopics.add(topic);\n      \n      // Find related terms\n      const related = findRelatedTerms(topic, [...nouns, ...phrases]);\n      \n      // Calculate confidence score\n      const confidence = calculateTopicConfidence(topic, content, related);\n      \n      topics.push({\n        topic,\n        confidence,\n        related: related.slice(0, 5) // Top 5 related terms\n      });\n    }\n  });\n  \n  return topics.sort((a, b) => b.confidence - a.confidence);\n}\n\nfunction calculateSemanticCoherence(tokens: string[]): number {\n  let coherenceScore = 0;\n  const windowSize = 5;\n  \n  // Calculate local coherence using sliding window\n  for (let i = 0; i < tokens.length - windowSize; i++) {\n    const window = tokens.slice(i, i + windowSize);\n    coherenceScore += calculateLocalCoherence(window);\n  }\n  \n  return Math.min(1, coherenceScore / Math.max(1, tokens.length - windowSize));\n}\n\nasync function calculateSemanticSimilarity(content: string): Promise<number> {\n  // Get historical content from database\n  const historicalContent = await prisma.contentAnalysis.findMany({\n    select: { originalContent: true },\n    take: 10,\n    orderBy: { createdAt: 'desc' }\n  });\n  \n  if (historicalContent.length === 0) return 1;\n  \n  // Calculate similarity with each historical content\n  const similarities = historicalContent.map((h: { originalContent: string }) => \n    calculateCosineSimilarity(\n      extractFeatureVector(content),\n      extractFeatureVector(h.originalContent)\n    )\n  );\n  \n  return Math.max(...similarities);\n}\n\nfunction calculateSentenceImportance(sentences: string[]): number[] {\n  const scores: number[] = [];\n  const localTfidf = new natural.TfIdf();\n  \n  // Add each sentence as a document\n  sentences.forEach(s => localTfidf.addDocument(s));\n  \n  // Calculate importance score for each sentence\n  sentences.forEach((sentence, i) => {\n    const words = tokenizer.tokenize(sentence) || [];\n    let score = 0;\n    \n    words.forEach(word => {\n      score += localTfidf.tfidf(word, i);\n    });\n    \n    scores.push(score / Math.max(1, words.length));\n  });\n  \n  return scores;\n}\n\nasync function generateExtractiveSummary(\n  sentences: string[],\n  scores: number[],\n  ratio: number\n): Promise<string> {\n  const numSentences = Math.max(1, Math.round(sentences.length * ratio));\n  \n  // Get top sentences\n  const topSentences = sentences\n    .map((sentence, index) => ({ sentence, score: scores[index] }))\n    .sort((a, b) => b.score - a.score)\n    .slice(0, numSentences)\n    .sort((a, b) => sentences.indexOf(a.sentence) - sentences.indexOf(b.sentence))\n    .map(item => item.sentence);\n  \n  return topSentences.join(' ');\n}\n\nasync function extractKeyPoints(content: string): Promise<string[]> {\n  const doc = nlp(content);\n  const sentences = doc.sentences().out('array');\n  const keyPoints: string[] = [];\n  \n  // Look for key indicators\n  sentences.forEach((sentence: string) => {\n    if (\n      sentence.includes('importantly') ||\n      sentence.includes('key') ||\n      sentence.includes('main') ||\n      sentence.includes('primary') ||\n      sentence.includes('crucial') ||\n      sentence.match(/first|second|third|finally/i)\n    ) {\n      keyPoints.push(sentence.trim());\n    }\n  });\n  \n  return keyPoints;\n}\n\nfunction calculateSummaryCoverage(original: string, summary: string): number {\n  const originalTokens = new Set(tokenizer.tokenize(original));\n  const summaryTokens = new Set(tokenizer.tokenize(summary));\n  \n  let covered = 0;\n  summaryTokens.forEach(token => {\n    if (originalTokens.has(token)) covered++;\n  });\n  \n  return covered / originalTokens.size;\n}\n\nfunction extractFeatureVector(text: string): number[] {\n  // Implement feature extraction (e.g., word embeddings, TF-IDF)\n  // This is a simplified version\n  const vector: number[] = new Array(100).fill(0);\n  const tokens = tokenizer.tokenize(text) || [];\n  \n  tokens.forEach((token, i) => {\n    vector[i % 100] += 1;\n  });\n  \n  return vector;\n}\n\nfunction calculateCosineSimilarity(v1: number[], v2: number[]): number {\n  const dotProduct = v1.reduce((sum, v, i) => sum + v * v2[i], 0);\n  const mag1 = Math.sqrt(v1.reduce((sum, v) => sum + v * v, 0));\n  const mag2 = Math.sqrt(v2.reduce((sum, v) => sum + v * v, 0));\n  return dotProduct / (mag1 * mag2);\n}\n\nfunction calculateLocalCoherence(window: string[]): number {\n  let coherence = 0;\n  \n  // Calculate average semantic similarity between adjacent words\n  for (let i = 0; i < window.length - 1; i++) {\n    coherence += 1 - (levenshtein(window[i], window[i + 1]) / Math.max(window[i].length, window[i + 1].length));\n  }\n  \n  return coherence / (window.length - 1);\n}\n\nfunction findRelatedTerms(topic: string, allTerms: string[]): string[] {\n  return allTerms\n    .filter(term => term !== topic)\n    .map(term => ({\n      term,\n      similarity: 1 - (levenshtein(topic, term) / Math.max(topic.length, term.length))\n    }))\n    .filter(item => item.similarity > 0.3)\n    .sort((a, b) => b.similarity - a.similarity)\n    .map(item => item.term);\n}\n\nfunction calculateTopicConfidence(topic: string, content: string, related: string[]): number {\n  // Calculate confidence based on frequency and related terms\n  const frequency = (content.match(new RegExp(topic, 'gi')) || []).length;\n  const relatedScore = related.length / 10; // Normalize to 0-1\n  \n  return Math.min(1, (frequency / 100) + (relatedScore * 0.5));\n}\n\nfunction determineKeywordType(\n  keyword: string,\n  entities: any[]\n): 'topic' | 'action' | 'entity' {\n  // Check if it's an entity\n  if (entities.some(e => e.text === keyword)) {\n    return 'entity';\n  }\n  \n  // Check if it's an action (verb)\n  const doc = nlp(keyword);\n  if (doc.verbs().length > 0) {\n    return 'action';\n  }\n  \n  // Default to topic\n  return 'topic';\n}\n\n// Add missing functions\n\nfunction extractTfIdfKeywords(content: string, tokens: string[]): Array<{term: string; score: number; frequency: number}> {\n  const localTfidf = new natural.TfIdf();\n  localTfidf.addDocument(content);\n  \n  return tokens\n    .map(term => ({\n      term,\n      score: localTfidf.tfidf(term, 0),\n      frequency: (content.match(new RegExp(term, 'gi')) || []).length\n    }))\n    .sort((a, b) => b.score - a.score)\n    .slice(0, 20); // Top 20 keywords\n}\n\nfunction extractTextRankPhrases(content: string, tokens: string[]): Array<{text: string; score: number; words: string[]}> {\n  const phrases: Array<{text: string; score: number; words: string[]}> = [];\n  \n  // Extract noun phrases using compromise\n  const doc = nlp(content);\n  const nounPhrases = doc.match('#Noun+ (#Preposition? #Noun+)?').out('array');\n  \n  // Score phrases using TF-IDF and length\n  nounPhrases.forEach((phrase: string) => {\n    const words = phrase.split(/\\s+/);\n    const phraseTfidf = new natural.TfIdf();\n    phraseTfidf.addDocument(phrase);\n    \n    const score = words.reduce((sum: number, word: string) => sum + phraseTfidf.tfidf(word, 0), 0) / words.length;\n    \n    phrases.push({\n      text: phrase,\n      score,\n      words\n    });\n  });\n  \n  return phrases\n    .sort((a, b) => b.score - a.score)\n    .slice(0, 10); // Top 10 phrases\n}\n\nfunction calculatePositionScores(tokens: string[]): Record<string, number[]> {\n  const positions: Record<string, number[]> = {};\n  \n  tokens.forEach((token, index) => {\n    if (!positions[token]) {\n      positions[token] = [];\n    }\n    positions[token].push(index);\n  });\n  \n  return positions;\n}\n\n// Export new NLP helpers\nexport const nlpHelpers = {\n  analyzeSemantics,\n  generateSummary,\n  extractEnhancedKeywords,\n  calculateSemanticSimilarity\n};\n\n// Add new functions for classification and advanced sentiment\n\n/**\n * Advanced text classification with hierarchical categories\n */\nexport async function classifyText(content: string): Promise<TextClassificationResult> {\n  try {\n    const doc = nlp(content);\n    const tokens = tokenizer.tokenize(content) || [];\n    \n    // Extract features for classification\n    const features = await extractClassificationFeatures(content, tokens);\n    \n    // Classify main category\n    const categoryScores = await classifyCategory(features);\n    const mainCategory = categoryScores[0];\n    \n    // Get subcategories based on main category\n    const subcategories = await classifySubcategories(features, mainCategory.category);\n    \n    // Extract relevant topics\n    const topics = await extractRelevantTopics(content, mainCategory.category);\n    \n    return {\n      category: mainCategory.category,\n      confidence: mainCategory.confidence,\n      subcategories: subcategories.map(sub => ({\n        name: sub.name,\n        confidence: sub.confidence\n      })),\n      topics: topics.map(topic => ({\n        name: topic.name,\n        relevance: topic.relevance\n      }))\n    };\n  } catch (error) {\n    const errorContext = {\n      error: error instanceof Error ? error.message : String(error),\n      contentLength: content.length\n    };\n    logger.error('Error in text classification', errorContext);\n    throw new Error('Failed to classify text');\n  }\n}\n\n/**\n * Advanced sentiment analysis with aspect-based and emotion detection\n */\nexport async function analyzeAdvancedSentiment(content: string): Promise<AdvancedSentimentResult> {\n  try {\n    const doc = nlp(content);\n    const sentences = doc.sentences().out('array');\n    \n    // Overall sentiment using ensemble approach\n    const overallSentiment = await calculateEnsembleSentiment(content);\n    \n    // Aspect-based sentiment analysis\n    const aspects = await extractAndAnalyzeAspects(content);\n    \n    // Fine-grained and base-level emotion analysis\n    const granularEmotions = analyzeGranularEmotions(content);\n    const emotions = aggregateToBaseEmotions(granularEmotions);\n    \n    // Additional sentiment metrics\n    const intensity = calculateSentimentIntensity(content);\n    const subjectivity = calculateSubjectivity(content);\n    const sarcasm = detectSarcasm(content);\n    \n    return {\n      overall: {\n        score: overallSentiment.score,\n        label: overallSentiment.label,\n        confidence: overallSentiment.confidence\n      },\n      aspects,\n      emotions,\n      granularEmotions,\n      intensity,\n      subjectivity,\n      sarcasm\n    };\n  } catch (error) {\n    const errorContext = {\n      error: error instanceof Error ? error.message : String(error),\n      contentLength: content.length\n    };\n    logger.error('Error in advanced sentiment analysis', errorContext);\n    throw new Error('Failed to analyze sentiment');\n  }\n}\n\n// Helper functions for classification\n\nasync function extractClassificationFeatures(content: string, tokens: string[]) {\n  // TF-IDF features\n  const localTfidf = new natural.TfIdf();\n  localTfidf.addDocument(content);\n  \n  // N-gram features\n  const bigrams = NGrams.bigrams(tokens);\n  const trigrams = NGrams.trigrams(tokens);\n  \n  // POS features\n  const doc = nlp(content);\n  const pos = doc.terms().out('tags');\n  \n  // Combine features\n  return {\n    tokens,\n    tfidf: tokens.map(t => ({ term: t, score: localTfidf.tfidf(t, 0) })),\n    ngrams: [...bigrams, ...trigrams].map(ng => ng.join(' ')),\n    pos\n  };\n}\n\nasync function classifyCategory(features: any): Promise<Array<{category: string; confidence: number}>> {\n  // Use logistic regression classifier\n  const scores = contentClassifier.getClassifications(features.tokens.join(' '));\n  \n  return scores\n    .map(s => ({\n      category: s.label,\n      confidence: Math.exp(s.value) / (1 + Math.exp(s.value)) // Convert to probability\n    }))\n    .sort((a, b) => b.confidence - a.confidence);\n}\n\nasync function classifySubcategories(\n  features: any,\n  mainCategory: string\n): Promise<Array<{name: string; confidence: number}>> {\n  // Get subcategories based on main category\n  const subcategories = getSubcategories(mainCategory);\n  \n  // Calculate confidence for each subcategory\n  const results = subcategories.map(sub => {\n    const confidence = calculateSubcategoryConfidence(features, sub);\n    return {\n      name: sub,\n      confidence\n    };\n  });\n  \n  return results\n    .sort((a, b) => b.confidence - a.confidence)\n    .slice(0, 3); // Top 3 subcategories\n}\n\nfunction getSubcategories(category: string): string[] {\n  // Define subcategories for each main category\n  const subcategoryMap: Record<string, string[]> = {\n    marketing: ['social-media', 'email', 'content', 'advertising', 'branding'],\n    technical: ['development', 'infrastructure', 'security', 'data', 'integration'],\n    financial: ['payments', 'banking', 'investment', 'compliance', 'risk'],\n    support: ['customer-service', 'technical-support', 'documentation', 'training']\n  };\n  \n  return subcategoryMap[category] || [];\n}\n\nfunction calculateSubcategoryConfidence(features: any, subcategory: string): number {\n  // Simple confidence calculation based on term frequency\n  const relevantTerms = _getRelevantTermsInternal(subcategory);\n  const termMatches = features.tokens.filter((token: string) => \n    relevantTerms.some(term => token.toLowerCase().includes(term))\n  ).length;\n  \n  return Math.min(1, termMatches / Math.max(1, features.tokens.length));\n}\n\nfunction _getRelevantTermsInternal(subcategory: string): string[] {\n  // Define relevant terms for each subcategory\n  const termMap: Record<string, string[]> = {\n    'social-media': ['social', 'media', 'post', 'engagement', 'followers'],\n    'email': ['email', 'newsletter', 'campaign', 'open', 'click'],\n    'content': ['content', 'blog', 'article', 'video', 'post'],\n    // Add more subcategories and terms as needed\n  };\n  \n  return termMap[subcategory] || [];\n}\n\n// Helper functions for advanced sentiment analysis\n\nasync function calculateEnsembleSentiment(content: string): Promise<{\n  score: number;\n  label: 'positive' | 'negative' | 'neutral';\n  confidence: number;\n}> {\n  // Get sentiment from multiple models\n  const vaderScore = sentimentAnalyzer.analyze(content).score;\n  const naturalScore = new NaturalSentiment('English', PorterStemmer, 'afinn').getSentiment(\n    tokenizer.tokenize(content) || []\n  );\n  \n  // Combine scores with weights\n  const combinedScore = (vaderScore * 0.6) + (naturalScore * 0.4);\n  \n  // Calculate confidence based on agreement\n  const confidence = 1 - Math.abs(vaderScore - naturalScore) / 2;\n  \n  return {\n    score: combinedScore,\n    label: combinedScore > 0.1 ? 'positive' : combinedScore < -0.1 ? 'negative' : 'neutral',\n    confidence\n  };\n}\n\nasync function extractAndAnalyzeAspects(content: string): Promise<Array<{\n  aspect: string;\n  sentiment: {\n    score: number;\n    label: 'positive' | 'negative' | 'neutral';\n  };\n  examples: string[];\n}>> {\n  const doc = nlp(content);\n  const aspects: Array<{\n    aspect: string;\n    sentiment: {\n      score: number;\n      label: 'positive' | 'negative' | 'neutral';\n    };\n    examples: string[];\n  }> = [];\n  \n  // Extract noun phrases as potential aspects\n  const nounPhrases = doc.match('#Noun+').out('array');\n  \n  for (const aspect of nounPhrases) {\n    // Find sentences containing the aspect\n    const relevantSentences = content\n      .split(/[.!?]+/)\n      .filter(s => s.toLowerCase().includes(aspect.toLowerCase()));\n    \n    if (relevantSentences.length > 0) {\n      // Calculate sentiment for each mention\n      const sentiments = await Promise.all(\n        relevantSentences.map(s => calculateEnsembleSentiment(s))\n      );\n      \n      // Average sentiment scores\n      const avgScore = sentiments.reduce((sum, s) => sum + s.score, 0) / sentiments.length;\n      \n      aspects.push({\n        aspect,\n        sentiment: {\n          score: avgScore,\n          label: avgScore > 0.1 ? 'positive' : avgScore < -0.1 ? 'negative' : 'neutral'\n        },\n        examples: relevantSentences.slice(0, 3) // Top 3 examples\n      });\n    }\n  }\n  \n  return aspects;\n}\n\nfunction analyzeGranularEmotions(content: string): Record<string, number> {\n  const sentences = content.split(/[.!?]+/);\n  // Initialise score map\n  const granularScores: Record<string, number> = {};\n  Object.keys(EMOTION_TRAINING_DATA).forEach(label => {\n    granularScores[label] = 0;\n  });\n\n  for (const sentence of sentences) {\n    const classifications = emotionClassifier.getClassifications(sentence);\n    classifications.forEach(c => {\n      if (granularScores[c.label] !== undefined) {\n        const prob = Math.exp(c.value) / (1 + Math.exp(c.value));\n        granularScores[c.label] += prob;\n      }\n    });\n  }\n\n  // Normalise\n  const total = Object.values(granularScores).reduce((sum, v) => sum + v, 0);\n  if (total > 0) {\n    Object.keys(granularScores).forEach(k => {\n      granularScores[k] /= total;\n    });\n  }\n\n  return granularScores;\n}\n\nfunction aggregateToBaseEmotions(granular: Record<string, number>): Record<string, number> {\n  const base: Record<string, number> = {\n    joy: 0,\n    sadness: 0,\n    anger: 0,\n    fear: 0,\n    surprise: 0,\n    trust: 0,\n    anticipation: 0,\n    disgust: 0\n  };\n\n  Object.entries(granular).forEach(([label, score]) => {\n    const mapped = GRANULAR_TO_BASE_MAP[label as keyof typeof GRANULAR_TO_BASE_MAP] as keyof typeof base | undefined;\n    if (mapped) {\n      base[mapped] += score;\n    } else if ((base as any)[label] !== undefined) {\n      // label already a base category\n      (base as any)[label] += score;\n    }\n  });\n\n  return base;\n}\n\nfunction calculateSentimentIntensity(content: string): number {\n  const intensifiers = ['very', 'extremely', 'absolutely', 'totally', 'completely'];\n  const exclamations = (content.match(/!/g) || []).length;\n  const upperCase = (content.match(/[A-Z]{2,}/g) || []).length;\n  const intensifierCount = intensifiers.reduce(\n    (count, word) => count + (content.toLowerCase().match(new RegExp(word, 'g')) || []).length,\n    0\n  );\n  \n  return Math.min(1, (intensifierCount * 0.2 + exclamations * 0.3 + upperCase * 0.1));\n}\n\nfunction calculateSubjectivity(content: string): number {\n  const doc = nlp(content);\n  const words = doc.terms().out('array');\n  \n  // Count subjective indicators\n  const personalPronouns = doc.match('(i|me|my|mine|we|us|our|ours)').out('array').length;\n  const opinions = doc.match('(think|feel|believe|assume|suppose)').out('array').length;\n  const adjectives = doc.match('#Adjective').out('array').length;\n  \n  return Math.min(1, (personalPronouns + opinions * 2 + adjectives) / words.length);\n}\n\nfunction detectSarcasm(content: string): { detected: boolean; confidence: number } {\n  const sarcasmIndicators = [\n    // Contrast between positive and negative\n    content.match(/\\b(great|awesome|fantastic|wonderful)\\b.*\\b(terrible|awful|horrible)\\b/i),\n    // Exaggeration\n    content.match(/\\b(obviously|clearly|totally|absolutely|definitely)\\b/gi),\n    // Question marks and exclamation points\n    content.match(/[!?]{2,}/g),\n    // Quotation marks for emphasis\n    content.match(/\"([^\"]*?)\"/g)\n  ];\n  \n  const indicatorCount = sarcasmIndicators.filter(i => i !== null).length;\n  const confidence = Math.min(1, indicatorCount * 0.25);\n  \n  return {\n    detected: confidence > 0.5,\n    confidence\n  };\n}\n\n// Initialize and train classifiers\nfunction initializeClassifiers() {\n  // Train emotion classifier\n  Object.entries(EMOTION_TRAINING_DATA).forEach(([emotion, examples]) => {\n    examples.forEach(example => {\n      emotionClassifier.addDocument(example, emotion);\n    });\n  });\n  emotionClassifier.train();\n  \n  // Train content classifier (example categories)\n  const contentCategories = {\n    marketing: ['campaign', 'promotion', 'advertisement', 'brand', 'market'],\n    technical: ['software', 'hardware', 'system', 'data', 'technology'],\n    financial: ['payment', 'transaction', 'money', 'finance', 'banking'],\n    support: ['help', 'assistance', 'problem', 'issue', 'resolution']\n  };\n  \n  Object.entries(contentCategories).forEach(([category, examples]) => {\n    examples.forEach(example => {\n      contentClassifier.addDocument(example, category);\n    });\n  });\n  contentClassifier.train();\n}\n\n// Initialize classifiers when module loads\ninitializeClassifiers();\n\n// Export new functions\nexport const advancedTextAnalysis = {\n  classifyText,\n  analyzeAdvancedSentiment,\n  detectSarcasm,\n  calculateSubjectivity\n};\n\n// Export main enhanced content intelligence object\nexport const enhancedContentIntelligence = {\n  enhancedSentimentAnalysis,\n  enhancedContentScoring,\n  ...nlpHelpers,\n  ...advancedTextAnalysis,\n  ...contentIntelligenceHelpers\n};\n\n// Add missing functions\n\nasync function extractRelevantTopics(content: string, category: string): Promise<Topic[]> {\n  const doc = nlp(content);\n  const localTfidf = new natural.TfIdf();\n  \n  // Add category-specific documents to TF-IDF\n  const categoryDocs = getCategoryDocuments(category);\n  categoryDocs.forEach(doc => localTfidf.addDocument(doc));\n  \n  // Add current content\n  localTfidf.addDocument(content);\n  \n  // Extract topics using TF-IDF scores\n  const terms = localTfidf.listTerms(0);\n  const topics = terms\n    .slice(0, 10)\n    .map(term => ({\n      name: term.term,\n      relevance: term.tfidf\n    }));\n  \n  return topics;\n}\n\nfunction getCategoryDocuments(category: string): string[] {\n  // Sample documents for each category\n  const categoryDocs: Record<string, string[]> = {\n    marketing: [\n      'digital marketing campaign brand awareness social media',\n      'marketing strategy customer engagement promotion advertising',\n      'market research consumer behavior brand positioning'\n    ],\n    technical: [\n      'software development system architecture database',\n      'technical documentation api integration deployment',\n      'hardware specifications system requirements maintenance'\n    ],\n    financial: [\n      'financial transactions payment processing banking',\n      'investment portfolio asset management trading',\n      'financial analysis risk assessment compliance'\n    ],\n    support: [\n      'customer support ticket resolution troubleshooting',\n      'technical assistance user guide documentation',\n      'support services maintenance customer care'\n    ]\n  };\n  \n  return categoryDocs[category] || [];\n}\n\n "],"names":["advancedTextAnalysis","analyzeAdvancedSentiment","analyzeSemantics","classifyText","contentIntelligenceHelpers","enhancedContentIntelligence","enhancedContentScoring","enhancedSentimentAnalysis","extractContentFeatures","extractEnhancedKeywords","generateSummary","nlpHelpers","tokenizer","natural","WordTokenizer","tfidf","TfIdf","sentimentAnalyzer","sentiment","BERT_CONFIG","modelPath","maxLength","batchSize","contentClassifier","LogisticRegressionClassifier","emotionClassifier","BayesClassifier","topicClassifier","EMOTION_TRAINING_DATA","joy","sadness","anger","fear","surprise","trust","anticipation","disgust","happiness","contentment","excitement","frustration","anxiety","wonder","confidence","optimism","contempt","GRANULAR_TO_BASE_MAP","content","contentType","tokens","tokenize","toLowerCase","doc","nlp","vaderResult","analyze","entities","topics","json","aspects","extractAspects","aspectSentiments","analyzeAspectSentiments","entitySentiments","analyzeEntitySentiments","calculateModelConfidence","score","comparative","positive","negative","emotions","analyzeEmotions","aspectBasedSentiment","entitySentiment","error","errorContext","Error","message","String","contentLength","length","logger","bigrams","NGrams","trigrams","addDocument","keyPhrases","getTopTfIdfTerms","readabilityMetrics","fleschKincaid","calculateFleschKincaid","gunningFog","calculateGunningFog","smog","calculateSMOG","automatedReadability","calculateARI","sentences","split","filter","s","trim","stylometricFeatures","avgSentenceLength","reduce","sum","avgWordLength","w","lexicalDiversity","Set","map","t","size","punctuationRatio","match","out","ngrams","ng","join","features","historicalData","getHistoricalPerformance","readabilityScore","calculateEnhancedReadabilityScore","engagementScore","predictEngagementScore","conversionScore","predictConversionScore","sentimentResult","improvements","strengths","generateMLBasedRecommendations","sentimentScore","overallScore","calculateOverallScore","nouns","results","aspect","relevantSentences","extractRelevantSentences","calculateAspectSentiment","calculateConfidenceScore","findRelatedAspects","entity","text","calculateEntitySentiment","count","calculateEmotionIntensity","emotion","scores","variance","calculateVariance","Math","min","numbers","mean","a","b","pow","sqrt","prisma","contentAnalysis","findMany","where","select","originalContent","result","orderBy","createdAt","take","words","syllables","word","countSyllables","wordsPerSentence","syllablesPerWord","complexWords","complexWordPercentage","characters","replace","charsPerWord","vowelGroups","n","terms","listTerms","forEach","item","push","term","sort","slice","round","sentence","includes","sentiments","log","allAspects","normalizeScore","max","extractTopics","coherence","calculateSemanticCoherence","keywordResults","similarity","calculateSemanticSimilarity","summaryResult","summary","shortSummary","keywords","k","type","determineKeywordType","name","topic","relatedTerms","related","semanticSimilarity","sentenceScores","calculateSentenceImportance","generateExtractiveSummary","longSummary","keyPoints","extractKeyPoints","coverage","calculateSummaryCoverage","tfidfKeywords","extractTfIdfKeywords","textRankPhrases","extractTextRankPhrases","positionScores","calculatePositionScores","frequency","position","phrases","processedTopics","has","add","findRelatedTerms","calculateTopicConfidence","coherenceScore","windowSize","i","window","calculateLocalCoherence","historicalContent","similarities","h","calculateCosineSimilarity","extractFeatureVector","localTfidf","ratio","numSentences","topSentences","index","indexOf","original","originalTokens","summaryTokens","covered","token","vector","Array","fill","v1","v2","dotProduct","v","mag1","mag2","levenshtein","allTerms","RegExp","relatedScore","keyword","some","e","verbs","nounPhrases","phrase","phraseTfidf","positions","extractClassificationFeatures","categoryScores","classifyCategory","mainCategory","subcategories","classifySubcategories","category","extractRelevantTopics","sub","relevance","overallSentiment","calculateEnsembleSentiment","extractAndAnalyzeAspects","granularEmotions","analyzeGranularEmotions","aggregateToBaseEmotions","intensity","calculateSentimentIntensity","subjectivity","calculateSubjectivity","sarcasm","detectSarcasm","overall","label","pos","getClassifications","exp","value","getSubcategories","calculateSubcategoryConfidence","subcategoryMap","marketing","technical","financial","support","subcategory","relevantTerms","_getRelevantTermsInternal","termMatches","termMap","vaderScore","naturalScore","NaturalSentiment","PorterStemmer","getSentiment","combinedScore","abs","Promise","all","avgScore","examples","granularScores","Object","keys","classifications","c","undefined","prob","total","values","granular","base","entries","mapped","intensifiers","exclamations","upperCase","intensifierCount","personalPronouns","opinions","adjectives","sarcasmIndicators","indicatorCount","detected","initializeClassifiers","example","train","contentCategories","categoryDocs","getCategoryDocuments"],"mappings":"AAAA;;;CAGC;;;;;;;;;;;IAu3CYA,oBAAoB;eAApBA;;IAnVSC,wBAAwB;eAAxBA;;IA9bAC,gBAAgB;eAAhBA;;IAmZAC,YAAY;eAAZA;;IArdTC,0BAA0B;eAA1BA;;IA21BAC,2BAA2B;eAA3BA;;IAxlCSC,sBAAsB;eAAtBA;;IApGAC,yBAAyB;eAAzBA;;IAsDAC,sBAAsB;eAAtBA;;IAkcAC,uBAAuB;eAAvBA;;IApCAC,eAAe;eAAfA;;IAsVTC,UAAU;eAAVA;;;iEA3+BO;kEACE;mEACN;wBACmB;+DAChB;sCAGqB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAIxC,uBAAuB;AACvB,MAAMC,YAAY,IAAIC,gBAAO,CAACC,aAAa;AAC3C,MAAMC,QAAQ,IAAIF,gBAAO,CAACG,KAAK;AAC/B,MAAMC,oBAAoB,IAAIC,kBAAS;AAEvC,mCAAmC;AACnC,MAAMC,cAAc;IAClBC,WAAW;IACXC,WAAW;IACXC,WAAW;AACb;AAiIA,yBAAyB;AACzB,MAAMC,oBAAoB,IAAIC,qCAA4B;AAC1D,MAAMC,oBAAoB,IAAIC,wBAAe;AAC7C,MAAMC,kBAAkB,IAAID,wBAAe;AAE3C,gCAAgC;AAChC,MAAME,wBAAwB;IAC5BC,KAAK;QAAC;QAAS;QAAW;QAAa;QAAW;KAAS;IAC3DC,SAAS;QAAC;QAAO;QAAgB;QAAW;QAAa;KAAS;IAClEC,OAAO;QAAC;QAAS;QAAW;QAAY;QAAa;KAAU;IAC/DC,MAAM;QAAC;QAAU;QAAU;QAAa;QAAW;KAAU;IAC7DC,UAAU;QAAC;QAAa;QAAU;QAAc;QAAW;KAAU;IACrEC,OAAO;QAAC;QAAS;QAAY;QAAc;QAAa;KAAW;IACnEC,cAAc;QAAC;QAAU;QAAc;QAAS;QAAmB;KAAU;IAC7EC,SAAS;QAAC;QAAa;QAAY;QAAY;QAAY;KAAY;IACvE,wBAAwB;IACxBC,WAAW;QAAC;QAAS;QAAU;QAAY;QAAW;KAAQ;IAC9DC,aAAa;QAAC;QAAW;QAAa;QAAU;QAAa;KAAU;IACvEC,YAAY;QAAC;QAAW;QAAY;QAAS;QAAgB;KAAS;IACtEC,aAAa;QAAC;QAAc;QAAW;QAAa;QAAS;KAAc;IAC3EC,SAAS;QAAC;QAAW;QAAW;QAAU;QAAY;KAAQ;IAC9DC,QAAQ;QAAC;QAAU;QAAO;QAAgB;QAAa;KAAS;IAChEC,YAAY;QAAC;QAAa;QAAW;QAAW;QAAU;KAAW;IACrEC,UAAU;QAAC;QAAc;QAAW;QAAc;QAAU;KAAW;IACvEC,UAAU;QAAC;QAAY;QAAS;QAAW;QAAa;KAAW;AACrE;AAEA,sEAAsE;AACtE,MAAMC,uBAA+F;IACnGT,WAAW;IACXC,aAAa;IACbC,YAAY;IACZC,aAAa;IACbC,SAAS;IACTC,QAAQ;IACRC,YAAY;IACZC,UAAU;IACVC,UAAU;AACZ;AAKO,eAAetC,0BACpBwC,OAAe,EACfC,WAAwB;IAExB,IAAI;QACF,0BAA0B;QAC1B,MAAMC,SAASrC,UAAUsC,QAAQ,CAACH,QAAQI,WAAW,OAAO,EAAE;QAC9D,MAAMC,MAAMC,IAAAA,mBAAG,EAACN;QAEhB,2CAA2C;QAC3C,MAAMO,cAAcrC,kBAAkBsC,OAAO,CAACR;QAE9C,yBAAyB;QACzB,MAAMS,WAAWJ,IAAIK,MAAM,GAAGC,IAAI;QAElC,0CAA0C;QAC1C,MAAMC,UAAU,MAAMC,eAAeb;QACrC,MAAMc,mBAAmB,MAAMC,wBAAwBf,SAASY;QAEhE,yBAAyB;QACzB,MAAMI,mBAAmB,MAAMC,wBAAwBjB,SAASS;QAEhE,gDAAgD;QAChD,MAAMb,aAAasB,yBAAyB;YAC1CX,YAAYY,KAAK;SAElB;QAED,kBAAkB;QAClB,OAAO;YACLA,OAAOZ,YAAYY,KAAK;YACxBC,aAAab,YAAYa,WAAW;YACpClB,QAAQA;YACRmB,UAAUd,YAAYc,QAAQ,IAAI,EAAE;YACpCC,UAAUf,YAAYe,QAAQ,IAAI,EAAE;YACpC1B,YAAYA;YACZ2B,UAAU,MAAMC,gBAAgBxB;YAChCyB,sBAAsBX;YACtBY,iBAAiBV;QACnB;IACF,EAAE,OAAOW,OAAO;QACd,MAAMC,eAAe;YACnBD,OAAOA,iBAAiBE,QAAQF,MAAMG,OAAO,GAAGC,OAAOJ;YACvD1B,aAAaA;YACb+B,eAAehC,QAAQiC,MAAM;QAC/B;QACAC,cAAM,CAACP,KAAK,CAAC,wCAAwCC;QACrD,MAAM,IAAIC,MAAM;IAClB;AACF;AAKO,eAAepE,uBAAuBuC,OAAe;IAC1D,MAAME,SAASrC,UAAUsC,QAAQ,CAACH,YAAY,EAAE;IAChD,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;IAEhB,mBAAmB;IACnB,MAAMmC,UAAUrE,gBAAO,CAACsE,MAAM,CAACD,OAAO,CAACjC;IACvC,MAAMmC,WAAWvE,gBAAO,CAACsE,MAAM,CAACC,QAAQ,CAACnC;IAEzC,mCAAmC;IACnClC,MAAMsE,WAAW,CAACtC;IAClB,MAAMuC,aAAaC,iBAAiBxE,OAAO;IAE3C,gCAAgC;IAChC,MAAMyE,qBAAqB;QACzBC,eAAeC,uBAAuB3C;QACtC4C,YAAYC,oBAAoB7C;QAChC8C,MAAMC,cAAc/C;QACpBgD,sBAAsBC,aAAajD;IACrC;IAEA,+BAA+B;IAC/B,MAAMkD,YAAYlD,QAAQmD,KAAK,CAAC,UAAUC,MAAM,CAACC,CAAAA,IAAKA,EAAEC,IAAI,GAAGrB,MAAM,GAAG;IAExE,iCAAiC;IACjC,MAAMsB,sBAAsB;QAC1BC,mBAAmBN,UAAUO,MAAM,CAAC,CAACC,KAAKL,IAAMK,MAAML,EAAEF,KAAK,CAAC,OAAOlB,MAAM,EAAE,KAAKiB,UAAUjB,MAAM;QAClG0B,eAAezD,OAAOuD,MAAM,CAAC,CAACC,KAAKE,IAAMF,MAAME,EAAE3B,MAAM,EAAE,KAAK/B,OAAO+B,MAAM;QAC3E4B,kBAAkB,IAAIC,IAAI5D,OAAO6D,GAAG,CAACC,CAAAA,IAAKA,EAAE5D,WAAW,KAAK6D,IAAI,GAAG/D,OAAO+B,MAAM;QAChFiC,kBAAkB,AAAClE,CAAAA,QAAQmE,KAAK,CAAC,gBAAgB,EAAE,AAAD,EAAGlC,MAAM,GAAGjC,QAAQiC,MAAM;IAC9E;IAEA,MAAMvB,SAASL,IAAIK,MAAM,GAAG0D,GAAG,CAAC,YAAY,EAAE;IAE9C,OAAO;QACLlE;QACAmE,QAAQ;eAAIlC;eAAYE;SAAS,CAAC0B,GAAG,CAACO,CAAAA,KAAMA,GAAGC,IAAI,CAAC;QACpDhC;QACA9B,UAAUC;QACV+B;QACAc;IACF;AACF;AAKO,eAAehG,uBACpByC,OAAe,EACfC,WAAwB;IAExB,IAAI;QACF,mBAAmB;QACnB,MAAMuE,WAAW,MAAM/G,uBAAuBuC;QAE9C,kCAAkC;QAClC,MAAMyE,iBAAiB,MAAMC,yBAAyBzE;QAEtD,6BAA6B;QAC7B,MAAM0E,mBAAmBC,kCAAkCJ;QAC3D,MAAMK,kBAAkB,MAAMC,uBAAuBN,UAAUC;QAC/D,MAAMM,kBAAkB,MAAMC,uBAAuBR,UAAUC;QAC/D,MAAMQ,kBAAkB,MAAMzH,0BAA0BwC,SAASC;QAEjE,iCAAiC;QACjC,MAAM,EAAEiF,YAAY,EAAEC,SAAS,EAAE,GAAG,MAAMC,+BACxCZ,UACA;YACEG;YACAE;YACAE;YACAM,gBAAgBJ,gBAAgB9D,KAAK;QACvC;QAGF,OAAO;YACLmE,cAAcC,sBAAsB;gBAClCZ;gBACAE;gBACAE;gBACAE,gBAAgB9D,KAAK;aACtB;YACDwD;YACAE;YACAE;YACAM,gBAAgBJ,gBAAgB9D,KAAK;YACrC+D;YACAC;QACF;IACF,EAAE,OAAOxD,OAAO;QACd,MAAMC,eAAe;YACnBD,OAAOA,iBAAiBE,QAAQF,MAAMG,OAAO,GAAGC,OAAOJ;YACvD1B,aAAaA;YACb+B,eAAehC,QAAQiC,MAAM;QAC/B;QACAC,cAAM,CAACP,KAAK,CAAC,qCAAqCC;QAClD,MAAM,IAAIC,MAAM;IAClB;AACF;AAEA,mBAAmB;AAEnB,eAAehB,eAAeb,OAAe;IAC3C,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;IAChB,OAAOK,IAAImF,KAAK,GAAGpB,GAAG,CAAC;AACzB;AAEA,eAAerD,wBAAwBf,OAAe,EAAEY,OAAiB;IACvE,MAAM6E,UAAe,CAAC;IACtB,KAAK,MAAMC,UAAU9E,QAAS;QAC5B,MAAM+E,oBAAoBC,yBAAyB5F,SAAS0F;QAC5DD,OAAO,CAACC,OAAO,GAAG;YAChBvE,OAAO,MAAM0E,yBAAyBF;YACtC/F,YAAYkG,yBAAyBH,kBAAkB1D,MAAM;YAC7DrB,SAASmF,mBAAmBL,QAAQ9E;QACtC;IACF;IACA,OAAO6E;AACT;AAEA,eAAexE,wBAAwBjB,OAAe,EAAES,QAAe;IACrE,OAAOA,SAASsD,GAAG,CAACiC,CAAAA,SAAW,CAAA;YAC7BA,QAAQA,OAAOC,IAAI;YACnB9H,WAAW+H,yBAAyBlG,SAASgG;YAC7CpG,YAAYkG,yBAAyBE,OAAOG,KAAK;QACnD,CAAA;AACF;AAEA,eAAe3E,gBAAgBxB,OAAe;IAC5C,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;IAChB,OAAO;QACLlB,KAAKsH,0BAA0B/F,KAAK;QACpCtB,SAASqH,0BAA0B/F,KAAK;QACxCrB,OAAOoH,0BAA0B/F,KAAK;QACtCpB,MAAMmH,0BAA0B/F,KAAK;QACrCnB,UAAUkH,0BAA0B/F,KAAK;IAC3C;AACF;AAEA,SAAS+F,0BAA0B/F,GAAQ,EAAEgG,OAAe;IAC1D,mDAAmD;IACnD,OAAO,KAAK,cAAc;AAC5B;AAEA,SAASnF,yBAAyBoF,MAAgB;IAChD,MAAMC,WAAWC,kBAAkBF;IACnC,OAAO,IAAIG,KAAKC,GAAG,CAACH,UAAU;AAChC;AAEA,SAASC,kBAAkBG,OAAiB;IAC1C,MAAMC,OAAOD,QAAQlD,MAAM,CAAC,CAACoD,GAAGC,IAAMD,IAAIC,KAAKH,QAAQ1E,MAAM;IAC7D,MAAMsE,WAAWI,QAAQlD,MAAM,CAAC,CAACoD,GAAGC,IAAMD,IAAIJ,KAAKM,GAAG,CAACD,IAAIF,MAAM,IAAI,KAAKD,QAAQ1E,MAAM;IACxF,OAAOwE,KAAKO,IAAI,CAACT;AACnB;AAEA,eAAe7B,yBAAyBzE,WAAwB;IAC9D,OAAO,MAAMgH,eAAM,CAACC,eAAe,CAACC,QAAQ,CAAC;QAC3CC,OAAO;YAAEnH;QAAY;QACrBoH,QAAQ;YACNC,iBAAiB;YACjBC,QAAQ;QACV;QACAC,SAAS;YAAEC,WAAW;QAAO;QAC7BC,MAAM;IACR;AACF;AAEA;;;CAGC,GACD,SAAS/E,uBAAuBsD,IAAY;IAC1C,MAAM/C,YAAY+C,KAAK9C,KAAK,CAAC,UAAUC,MAAM,CAACC,CAAAA,IAAKA,EAAEC,IAAI,GAAGrB,MAAM,GAAG;IACrE,MAAM0F,QAAQ1B,KAAK9C,KAAK,CAAC,OAAOC,MAAM,CAACQ,CAAAA,IAAKA,EAAE3B,MAAM,GAAG;IACvD,MAAM2F,YAAYD,MAAMlE,MAAM,CAAC,CAAC0C,OAAO0B,OAAS1B,QAAQ2B,eAAeD,OAAO;IAE9E,IAAI3E,UAAUjB,MAAM,KAAK,KAAK0F,MAAM1F,MAAM,KAAK,GAAG,OAAO;IAEzD,MAAM8F,mBAAmBJ,MAAM1F,MAAM,GAAGiB,UAAUjB,MAAM;IACxD,MAAM+F,mBAAmBJ,YAAYD,MAAM1F,MAAM;IAEjD,OAAO,OAAO8F,mBAAmB,OAAOC,mBAAmB;AAC7D;AAEA;;;CAGC,GACD,SAASnF,oBAAoBoD,IAAY;IACvC,MAAM/C,YAAY+C,KAAK9C,KAAK,CAAC,UAAUC,MAAM,CAACC,CAAAA,IAAKA,EAAEC,IAAI,GAAGrB,MAAM,GAAG;IACrE,MAAM0F,QAAQ1B,KAAK9C,KAAK,CAAC,OAAOC,MAAM,CAACQ,CAAAA,IAAKA,EAAE3B,MAAM,GAAG;IACvD,MAAMgG,eAAeN,MAAMvE,MAAM,CAACyE,CAAAA,OAAQC,eAAeD,QAAQ;IAEjE,IAAI3E,UAAUjB,MAAM,KAAK,KAAK0F,MAAM1F,MAAM,KAAK,GAAG,OAAO;IAEzD,MAAM8F,mBAAmBJ,MAAM1F,MAAM,GAAGiB,UAAUjB,MAAM;IACxD,MAAMiG,wBAAwB,AAACD,aAAahG,MAAM,GAAG0F,MAAM1F,MAAM,GAAI;IAErE,OAAO,MAAO8F,CAAAA,mBAAmBG,qBAAoB;AACvD;AAEA;;;CAGC,GACD,SAASnF,cAAckD,IAAY;IACjC,MAAM/C,YAAY+C,KAAK9C,KAAK,CAAC,UAAUC,MAAM,CAACC,CAAAA,IAAKA,EAAEC,IAAI,GAAGrB,MAAM,GAAG;IACrE,MAAM0F,QAAQ1B,KAAK9C,KAAK,CAAC,OAAOC,MAAM,CAACQ,CAAAA,IAAKA,EAAE3B,MAAM,GAAG;IACvD,MAAMgG,eAAeN,MAAMvE,MAAM,CAACyE,CAAAA,OAAQC,eAAeD,QAAQ;IAEjE,IAAI3E,UAAUjB,MAAM,KAAK,GAAG,OAAO;IAEnC,OAAO,QAAQwE,KAAKO,IAAI,CAAC,AAAC,KAAKiB,aAAahG,MAAM,GAAIiB,UAAUjB,MAAM,IAAI;AAC5E;AAEA;;;CAGC,GACD,SAASgB,aAAagD,IAAY;IAChC,MAAM/C,YAAY+C,KAAK9C,KAAK,CAAC,UAAUC,MAAM,CAACC,CAAAA,IAAKA,EAAEC,IAAI,GAAGrB,MAAM,GAAG;IACrE,MAAM0F,QAAQ1B,KAAK9C,KAAK,CAAC,OAAOC,MAAM,CAACQ,CAAAA,IAAKA,EAAE3B,MAAM,GAAG;IACvD,MAAMkG,aAAalC,KAAKmC,OAAO,CAAC,QAAQ,IAAInG,MAAM;IAElD,IAAIiB,UAAUjB,MAAM,KAAK,KAAK0F,MAAM1F,MAAM,KAAK,GAAG,OAAO;IAEzD,MAAMoG,eAAeF,aAAaR,MAAM1F,MAAM;IAC9C,MAAM8F,mBAAmBJ,MAAM1F,MAAM,GAAGiB,UAAUjB,MAAM;IAExD,OAAO,OAAOoG,eAAe,MAAMN,mBAAmB;AACxD;AAEA;;CAEC,GACD,SAASD,eAAeD,IAAY;IAClCA,OAAOA,KAAKzH,WAAW,GAAGgI,OAAO,CAAC,WAAW;IAC7C,IAAIP,KAAK5F,MAAM,IAAI,GAAG,OAAO;IAE7B,sCAAsC;IACtC4F,OAAOA,KAAKO,OAAO,CAAC,MAAM;IAE1B,qBAAqB;IACrB,MAAME,cAAcT,KAAK1D,KAAK,CAAC;IAC/B,OAAOmE,cAAcA,YAAYrG,MAAM,GAAG;AAC5C;AAEA;;CAEC,GACD,SAASO,iBAAiBxE,KAAoB,EAAEuK,CAAS;IACvD,MAAMC,QAA8C,EAAE;IAEtDxK,MAAMyK,SAAS,CAAC,GAAGC,OAAO,CAACC,CAAAA;QACzBH,MAAMI,IAAI,CAAC;YAAEC,MAAMF,KAAKE,IAAI;YAAE1H,OAAOwH,KAAK3K,KAAK;QAAC;IAClD;IAEA,OAAOwK,MACJM,IAAI,CAAC,CAACjC,GAAGC,IAAMA,EAAE3F,KAAK,GAAG0F,EAAE1F,KAAK,EAChC4H,KAAK,CAAC,GAAGR,GACTxE,GAAG,CAAC4E,CAAAA,OAAQA,KAAKE,IAAI;AAC1B;AAEA,eAAe/D,uBACbN,QAAyB,EACzBC,cAAqB;IAErB,2CAA2C;IAC3C,OAAO,IAAI,cAAc;AAC3B;AAEA,eAAeO,uBACbR,QAAyB,EACzBC,cAAqB;IAErB,2CAA2C;IAC3C,OAAO,IAAI,cAAc;AAC3B;AAEA,eAAeW,+BACbZ,QAAyB,EACzB8B,MAKC;IAED,+CAA+C;IAC/C,OAAO;QACLpB,cAAc,EAAE;QAChBC,WAAW,EAAE;IACf;AACF;AAEA,SAASI,sBAAsBe,MAAgB;IAC7C,OAAOG,KAAKuC,KAAK,CAAC1C,OAAO7C,MAAM,CAAC,CAACoD,GAAGC,IAAMD,IAAIC,KAAKR,OAAOrE,MAAM;AAClE;AAGO,MAAM5E,6BAA6B;IACxCI;IACA+D;IACAN;AACF;AAEA,+BAA+B;AAC/B,SAAS0E,yBAAyB5F,OAAe,EAAE0F,MAAc;IAC/D,OAAO1F,QACJmD,KAAK,CAAC,UACNC,MAAM,CAAC6F,CAAAA,WAAYA,SAAS7I,WAAW,GAAG8I,QAAQ,CAACxD,OAAOtF,WAAW;AAC1E;AAEA,eAAeyF,yBAAyB3C,SAAmB;IACzD,IAAIA,UAAUjB,MAAM,KAAK,GAAG,OAAO;IAEnC,MAAMkH,aAAajG,UAAUa,GAAG,CAACV,CAAAA,IAAKnF,kBAAkBsC,OAAO,CAAC6C,GAAGlC,KAAK;IACxE,OAAOgI,WAAW1F,MAAM,CAAC,CAACoD,GAAGC,IAAMD,IAAIC,GAAG,KAAKqC,WAAWlH,MAAM;AAClE;AAEA,SAAS6D,yBAAyBK,KAAa;IAC7C,OAAOM,KAAKC,GAAG,CAAC,GAAGD,KAAK2C,GAAG,CAACjD,QAAQ,KAAKM,KAAK2C,GAAG,CAAC;AACpD;AAEA,SAASrD,mBAAmBL,MAAc,EAAE2D,UAAoB;IAC9D,OAAOA,WAAWjG,MAAM,CAACyD,CAAAA,IACvBA,MAAMnB,UACLmB,CAAAA,EAAEqC,QAAQ,CAACxD,WAAWA,OAAOwD,QAAQ,CAACrC,EAAC;AAE5C;AAEA,SAASX,yBAAyBlG,OAAe,EAAEgG,MAAW;IAC5D,MAAML,oBAAoBC,yBAAyB5F,SAASgG,OAAOC,IAAI;IACvE,MAAMkD,aAAaxD,kBAAkB5B,GAAG,CAACV,CAAAA,IAAKnF,kBAAkBsC,OAAO,CAAC6C,GAAGlC,KAAK;IAChF,OAAOgI,WAAWlH,MAAM,GAAG,IACvBkH,WAAW1F,MAAM,CAAC,CAACoD,GAAGC,IAAMD,IAAIC,GAAG,KAAKqC,WAAWlH,MAAM,GACzD;AACN;AAEA,SAAS2C,kCAAkCJ,QAAyB;IAClE,MAAM,EACJ9B,aAAa,EACbE,UAAU,EACVE,IAAI,EACJE,oBAAoB,EACrB,GAAGwB,SAAS/B,kBAAkB;IAE/B,sDAAsD;IACtD,OAAOgE,KAAKuC,KAAK,CACf,AAACM,CAAAA,eAAe5G,iBACf4G,eAAe1G,cACf0G,eAAexG,QACfwG,eAAetG,qBAAoB,IAAK;AAE7C;AAEA,SAASsG,eAAenI,KAAa;IACnC,iCAAiC;IACjC,OAAOsF,KAAK8C,GAAG,CAAC,GAAG9C,KAAKC,GAAG,CAAC,KAAM,MAAMvF,QAAQ;AAClD;AAOO,eAAehE,iBAAiB6C,OAAe;IACpD,IAAI;QACF,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;QAChB,MAAME,SAASrC,UAAUsC,QAAQ,CAACH,YAAY,EAAE;QAEhD,8BAA8B;QAC9B,MAAMU,SAAS,MAAM8I,cAAcxJ;QACnC,MAAMS,WAAWJ,IAAIK,MAAM,GAAGC,IAAI;QAElC,+BAA+B;QAC/B,MAAM8I,YAAYC,2BAA2BxJ;QAE7C,oCAAoC;QACpC,MAAMyJ,iBAAiB,MAAMjM,wBAAwBsC;QAErD,wDAAwD;QACxD,MAAM4J,aAAa,MAAMC,4BAA4B7J;QAErD,kDAAkD;QAClD,MAAM8J,gBAAgB,MAAMnM,gBAAgBqC;QAE5C,OAAO;YACL+J,SAASD,cAAcE,YAAY;YACnCC,UAAUN,eAAeM,QAAQ,CAAClG,GAAG,CAACmG,CAAAA,IAAM,CAAA;oBAC1CrC,MAAMqC,EAAErB,IAAI;oBACZ1H,OAAO+I,EAAE/I,KAAK;oBACdgJ,MAAMC,qBAAqBF,EAAErB,IAAI,EAAEpI;gBACrC,CAAA;YACAC,QAAQA,OAAOqD,GAAG,CAACC,CAAAA,IAAM,CAAA;oBACvBqG,MAAMrG,EAAEsG,KAAK;oBACb1K,YAAYoE,EAAEpE,UAAU;oBACxB2K,cAAcvG,EAAEwG,OAAO;gBACzB,CAAA;YACAC,oBAAoBb;YACpBH;QACF;IACF,EAAE,OAAO9H,OAAO;QACd,MAAMC,eAAe;YACnBD,OAAOA,iBAAiBE,QAAQF,MAAMG,OAAO,GAAGC,OAAOJ;YACvDK,eAAehC,QAAQiC,MAAM;QAC/B;QACAC,cAAM,CAACP,KAAK,CAAC,8BAA8BC;QAC3C,MAAM,IAAIC,MAAM;IAClB;AACF;AAKO,eAAelE,gBAAgBqC,OAAe;IACnD,IAAI;QACF,MAAMkD,YAAYlD,QAAQmD,KAAK,CAAC,UAAUC,MAAM,CAACC,CAAAA,IAAKA,EAAEC,IAAI,GAAGrB,MAAM,GAAG;QAExE,uCAAuC;QACvC,MAAMyI,iBAAiBC,4BAA4BzH;QAEnD,0CAA0C;QAC1C,MAAM8G,eAAe,MAAMY,0BAA0B1H,WAAWwH,gBAAgB;QAChF,MAAMG,cAAc,MAAMD,0BAA0B1H,WAAWwH,gBAAgB;QAE/E,0CAA0C;QAC1C,MAAMI,YAAY,MAAMC,iBAAiB/K;QAEzC,2BAA2B;QAC3B,MAAMgL,WAAWC,yBAAyBjL,SAASgK;QAEnD,OAAO;YACLA;YACAa;YACAC;YACAE;QACF;IACF,EAAE,OAAOrJ,OAAO;QACd,MAAMC,eAAe;YACnBD,OAAOA,iBAAiBE,QAAQF,MAAMG,OAAO,GAAGC,OAAOJ;YACvDK,eAAehC,QAAQiC,MAAM;QAC/B;QACAC,cAAM,CAACP,KAAK,CAAC,4BAA4BC;QACzC,MAAM,IAAIC,MAAM;IAClB;AACF;AAKO,eAAenE,wBAAwBsC,OAAe;IAC3D,IAAI;QACF,MAAME,SAASrC,UAAUsC,QAAQ,CAACH,YAAY,EAAE;QAChD,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;QAEhB,kCAAkC;QAClC,MAAMkL,gBAAgBC,qBAAqBnL,SAASE;QAEpD,mCAAmC;QACnC,MAAMkL,kBAAkBC,uBAAuBrL,SAASE;QAExD,yBAAyB;QACzB,MAAMoL,iBAAiBC,wBAAwBrL;QAE/C,kBAAkB;QAClB,OAAO;YACL+J,UAAUiB,cAAcnH,GAAG,CAACmG,CAAAA,IAAM,CAAA;oBAChCrB,MAAMqB,EAAErB,IAAI;oBACZ1H,OAAO+I,EAAE/I,KAAK;oBACdqK,WAAWtB,EAAEsB,SAAS;oBACtBC,UAAUH,cAAc,CAACpB,EAAErB,IAAI,CAAC,IAAI,EAAE;gBACxC,CAAA;YACA6C,SAASN;QACX;IACF,EAAE,OAAOzJ,OAAO;QACd,MAAMC,eAAe;YACnBD,OAAOA,iBAAiBE,QAAQF,MAAMG,OAAO,GAAGC,OAAOJ;YACvDK,eAAehC,QAAQiC,MAAM;QAC/B;QACAC,cAAM,CAACP,KAAK,CAAC,6BAA6BC;QAC1C,MAAM,IAAIC,MAAM;IAClB;AACF;AAEA,wCAAwC;AAExC,eAAe2H,cAAcxJ,OAAe;IAC1C,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;IAChB,MAAMU,SAAwE,EAAE;IAEhF,iCAAiC;IACjC,MAAM8E,QAAQnF,IAAImF,KAAK,GAAGpB,GAAG,CAAC;IAC9B,MAAMsH,UAAUrL,IAAI8D,KAAK,CAAC,kCAAkCC,GAAG,CAAC;IAEhE,sCAAsC;IACtC,MAAMpG,QAAQ,IAAIF,gBAAO,CAACG,KAAK;IAC/BD,MAAMsE,WAAW,CAACtC;IAElB,+BAA+B;IAC/B,MAAM2L,kBAAkB,IAAI7H;IAC5B;WAAI0B;WAAUkG;KAAQ,CAAChD,OAAO,CAAC4B,CAAAA;QAC7B,IAAI,CAACqB,gBAAgBC,GAAG,CAACtB,QAAQ;YAC/BqB,gBAAgBE,GAAG,CAACvB;YAEpB,qBAAqB;YACrB,MAAME,UAAUsB,iBAAiBxB,OAAO;mBAAI9E;mBAAUkG;aAAQ;YAE9D,6BAA6B;YAC7B,MAAM9L,aAAamM,yBAAyBzB,OAAOtK,SAASwK;YAE5D9J,OAAOkI,IAAI,CAAC;gBACV0B;gBACA1K;gBACA4K,SAASA,QAAQzB,KAAK,CAAC,GAAG,GAAG,sBAAsB;YACrD;QACF;IACF;IAEA,OAAOrI,OAAOoI,IAAI,CAAC,CAACjC,GAAGC,IAAMA,EAAElH,UAAU,GAAGiH,EAAEjH,UAAU;AAC1D;AAEA,SAAS8J,2BAA2BxJ,MAAgB;IAClD,IAAI8L,iBAAiB;IACrB,MAAMC,aAAa;IAEnB,iDAAiD;IACjD,IAAK,IAAIC,IAAI,GAAGA,IAAIhM,OAAO+B,MAAM,GAAGgK,YAAYC,IAAK;QACnD,MAAMC,SAASjM,OAAO6I,KAAK,CAACmD,GAAGA,IAAID;QACnCD,kBAAkBI,wBAAwBD;IAC5C;IAEA,OAAO1F,KAAKC,GAAG,CAAC,GAAGsF,iBAAiBvF,KAAK8C,GAAG,CAAC,GAAGrJ,OAAO+B,MAAM,GAAGgK;AAClE;AAEA,eAAepC,4BAA4B7J,OAAe;IACxD,uCAAuC;IACvC,MAAMqM,oBAAoB,MAAMpF,eAAM,CAACC,eAAe,CAACC,QAAQ,CAAC;QAC9DE,QAAQ;YAAEC,iBAAiB;QAAK;QAChCI,MAAM;QACNF,SAAS;YAAEC,WAAW;QAAO;IAC/B;IAEA,IAAI4E,kBAAkBpK,MAAM,KAAK,GAAG,OAAO;IAE3C,oDAAoD;IACpD,MAAMqK,eAAeD,kBAAkBtI,GAAG,CAAC,CAACwI,IAC1CC,0BACEC,qBAAqBzM,UACrByM,qBAAqBF,EAAEjF,eAAe;IAI1C,OAAOb,KAAK8C,GAAG,IAAI+C;AACrB;AAEA,SAAS3B,4BAA4BzH,SAAmB;IACtD,MAAMoD,SAAmB,EAAE;IAC3B,MAAMoG,aAAa,IAAI5O,gBAAO,CAACG,KAAK;IAEpC,kCAAkC;IAClCiF,UAAUwF,OAAO,CAACrF,CAAAA,IAAKqJ,WAAWpK,WAAW,CAACe;IAE9C,+CAA+C;IAC/CH,UAAUwF,OAAO,CAAC,CAACO,UAAUiD;QAC3B,MAAMvE,QAAQ9J,UAAUsC,QAAQ,CAAC8I,aAAa,EAAE;QAChD,IAAI9H,QAAQ;QAEZwG,MAAMe,OAAO,CAACb,CAAAA;YACZ1G,SAASuL,WAAW1O,KAAK,CAAC6J,MAAMqE;QAClC;QAEA5F,OAAOsC,IAAI,CAACzH,QAAQsF,KAAK8C,GAAG,CAAC,GAAG5B,MAAM1F,MAAM;IAC9C;IAEA,OAAOqE;AACT;AAEA,eAAesE,0BACb1H,SAAmB,EACnBoD,MAAgB,EAChBqG,KAAa;IAEb,MAAMC,eAAenG,KAAK8C,GAAG,CAAC,GAAG9C,KAAKuC,KAAK,CAAC9F,UAAUjB,MAAM,GAAG0K;IAE/D,oBAAoB;IACpB,MAAME,eAAe3J,UAClBa,GAAG,CAAC,CAACkF,UAAU6D,QAAW,CAAA;YAAE7D;YAAU9H,OAAOmF,MAAM,CAACwG,MAAM;QAAC,CAAA,GAC3DhE,IAAI,CAAC,CAACjC,GAAGC,IAAMA,EAAE3F,KAAK,GAAG0F,EAAE1F,KAAK,EAChC4H,KAAK,CAAC,GAAG6D,cACT9D,IAAI,CAAC,CAACjC,GAAGC,IAAM5D,UAAU6J,OAAO,CAAClG,EAAEoC,QAAQ,IAAI/F,UAAU6J,OAAO,CAACjG,EAAEmC,QAAQ,GAC3ElF,GAAG,CAAC4E,CAAAA,OAAQA,KAAKM,QAAQ;IAE5B,OAAO4D,aAAatI,IAAI,CAAC;AAC3B;AAEA,eAAewG,iBAAiB/K,OAAe;IAC7C,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;IAChB,MAAMkD,YAAY7C,IAAI6C,SAAS,GAAGkB,GAAG,CAAC;IACtC,MAAM0G,YAAsB,EAAE;IAE9B,0BAA0B;IAC1B5H,UAAUwF,OAAO,CAAC,CAACO;QACjB,IACEA,SAASC,QAAQ,CAAC,kBAClBD,SAASC,QAAQ,CAAC,UAClBD,SAASC,QAAQ,CAAC,WAClBD,SAASC,QAAQ,CAAC,cAClBD,SAASC,QAAQ,CAAC,cAClBD,SAAS9E,KAAK,CAAC,gCACf;YACA2G,UAAUlC,IAAI,CAACK,SAAS3F,IAAI;QAC9B;IACF;IAEA,OAAOwH;AACT;AAEA,SAASG,yBAAyB+B,QAAgB,EAAEjD,OAAe;IACjE,MAAMkD,iBAAiB,IAAInJ,IAAIjG,UAAUsC,QAAQ,CAAC6M;IAClD,MAAME,gBAAgB,IAAIpJ,IAAIjG,UAAUsC,QAAQ,CAAC4J;IAEjD,IAAIoD,UAAU;IACdD,cAAcxE,OAAO,CAAC0E,CAAAA;QACpB,IAAIH,eAAerB,GAAG,CAACwB,QAAQD;IACjC;IAEA,OAAOA,UAAUF,eAAehJ,IAAI;AACtC;AAEA,SAASwI,qBAAqBxG,IAAY;IACxC,+DAA+D;IAC/D,+BAA+B;IAC/B,MAAMoH,SAAmB,IAAIC,MAAM,KAAKC,IAAI,CAAC;IAC7C,MAAMrN,SAASrC,UAAUsC,QAAQ,CAAC8F,SAAS,EAAE;IAE7C/F,OAAOwI,OAAO,CAAC,CAAC0E,OAAOlB;QACrBmB,MAAM,CAACnB,IAAI,IAAI,IAAI;IACrB;IAEA,OAAOmB;AACT;AAEA,SAASb,0BAA0BgB,EAAY,EAAEC,EAAY;IAC3D,MAAMC,aAAaF,GAAG/J,MAAM,CAAC,CAACC,KAAKiK,GAAGzB,IAAMxI,MAAMiK,IAAIF,EAAE,CAACvB,EAAE,EAAE;IAC7D,MAAM0B,OAAOnH,KAAKO,IAAI,CAACwG,GAAG/J,MAAM,CAAC,CAACC,KAAKiK,IAAMjK,MAAMiK,IAAIA,GAAG;IAC1D,MAAME,OAAOpH,KAAKO,IAAI,CAACyG,GAAGhK,MAAM,CAAC,CAACC,KAAKiK,IAAMjK,MAAMiK,IAAIA,GAAG;IAC1D,OAAOD,aAAcE,CAAAA,OAAOC,IAAG;AACjC;AAEA,SAASzB,wBAAwBD,MAAgB;IAC/C,IAAI1C,YAAY;IAEhB,+DAA+D;IAC/D,IAAK,IAAIyC,IAAI,GAAGA,IAAIC,OAAOlK,MAAM,GAAG,GAAGiK,IAAK;QAC1CzC,aAAa,IAAKqE,IAAAA,8BAAW,EAAC3B,MAAM,CAACD,EAAE,EAAEC,MAAM,CAACD,IAAI,EAAE,IAAIzF,KAAK8C,GAAG,CAAC4C,MAAM,CAACD,EAAE,CAACjK,MAAM,EAAEkK,MAAM,CAACD,IAAI,EAAE,CAACjK,MAAM;IAC3G;IAEA,OAAOwH,YAAa0C,CAAAA,OAAOlK,MAAM,GAAG,CAAA;AACtC;AAEA,SAAS6J,iBAAiBxB,KAAa,EAAEyD,QAAkB;IACzD,OAAOA,SACJ3K,MAAM,CAACyF,CAAAA,OAAQA,SAASyB,OACxBvG,GAAG,CAAC8E,CAAAA,OAAS,CAAA;YACZA;YACAe,YAAY,IAAKkE,IAAAA,8BAAW,EAACxD,OAAOzB,QAAQpC,KAAK8C,GAAG,CAACe,MAAMrI,MAAM,EAAE4G,KAAK5G,MAAM;QAChF,CAAA,GACCmB,MAAM,CAACuF,CAAAA,OAAQA,KAAKiB,UAAU,GAAG,KACjCd,IAAI,CAAC,CAACjC,GAAGC,IAAMA,EAAE8C,UAAU,GAAG/C,EAAE+C,UAAU,EAC1C7F,GAAG,CAAC4E,CAAAA,OAAQA,KAAKE,IAAI;AAC1B;AAEA,SAASkD,yBAAyBzB,KAAa,EAAEtK,OAAe,EAAEwK,OAAiB;IACjF,4DAA4D;IAC5D,MAAMgB,YAAY,AAACxL,CAAAA,QAAQmE,KAAK,CAAC,IAAI6J,OAAO1D,OAAO,UAAU,EAAE,AAAD,EAAGrI,MAAM;IACvE,MAAMgM,eAAezD,QAAQvI,MAAM,GAAG,IAAI,mBAAmB;IAE7D,OAAOwE,KAAKC,GAAG,CAAC,GAAG,AAAC8E,YAAY,MAAQyC,eAAe;AACzD;AAEA,SAAS7D,qBACP8D,OAAe,EACfzN,QAAe;IAEf,0BAA0B;IAC1B,IAAIA,SAAS0N,IAAI,CAACC,CAAAA,IAAKA,EAAEnI,IAAI,KAAKiI,UAAU;QAC1C,OAAO;IACT;IAEA,iCAAiC;IACjC,MAAM7N,MAAMC,IAAAA,mBAAG,EAAC4N;IAChB,IAAI7N,IAAIgO,KAAK,GAAGpM,MAAM,GAAG,GAAG;QAC1B,OAAO;IACT;IAEA,mBAAmB;IACnB,OAAO;AACT;AAEA,wBAAwB;AAExB,SAASkJ,qBAAqBnL,OAAe,EAAEE,MAAgB;IAC7D,MAAMwM,aAAa,IAAI5O,gBAAO,CAACG,KAAK;IACpCyO,WAAWpK,WAAW,CAACtC;IAEvB,OAAOE,OACJ6D,GAAG,CAAC8E,CAAAA,OAAS,CAAA;YACZA;YACA1H,OAAOuL,WAAW1O,KAAK,CAAC6K,MAAM;YAC9B2C,WAAW,AAACxL,CAAAA,QAAQmE,KAAK,CAAC,IAAI6J,OAAOnF,MAAM,UAAU,EAAE,AAAD,EAAG5G,MAAM;QACjE,CAAA,GACC6G,IAAI,CAAC,CAACjC,GAAGC,IAAMA,EAAE3F,KAAK,GAAG0F,EAAE1F,KAAK,EAChC4H,KAAK,CAAC,GAAG,KAAK,kBAAkB;AACrC;AAEA,SAASsC,uBAAuBrL,OAAe,EAAEE,MAAgB;IAC/D,MAAMwL,UAAiE,EAAE;IAEzE,wCAAwC;IACxC,MAAMrL,MAAMC,IAAAA,mBAAG,EAACN;IAChB,MAAMsO,cAAcjO,IAAI8D,KAAK,CAAC,kCAAkCC,GAAG,CAAC;IAEpE,wCAAwC;IACxCkK,YAAY5F,OAAO,CAAC,CAAC6F;QACnB,MAAM5G,QAAQ4G,OAAOpL,KAAK,CAAC;QAC3B,MAAMqL,cAAc,IAAI1Q,gBAAO,CAACG,KAAK;QACrCuQ,YAAYlM,WAAW,CAACiM;QAExB,MAAMpN,QAAQwG,MAAMlE,MAAM,CAAC,CAACC,KAAamE,OAAiBnE,MAAM8K,YAAYxQ,KAAK,CAAC6J,MAAM,IAAI,KAAKF,MAAM1F,MAAM;QAE7GyJ,QAAQ9C,IAAI,CAAC;YACX3C,MAAMsI;YACNpN;YACAwG;QACF;IACF;IAEA,OAAO+D,QACJ5C,IAAI,CAAC,CAACjC,GAAGC,IAAMA,EAAE3F,KAAK,GAAG0F,EAAE1F,KAAK,EAChC4H,KAAK,CAAC,GAAG,KAAK,iBAAiB;AACpC;AAEA,SAASwC,wBAAwBrL,MAAgB;IAC/C,MAAMuO,YAAsC,CAAC;IAE7CvO,OAAOwI,OAAO,CAAC,CAAC0E,OAAON;QACrB,IAAI,CAAC2B,SAAS,CAACrB,MAAM,EAAE;YACrBqB,SAAS,CAACrB,MAAM,GAAG,EAAE;QACvB;QACAqB,SAAS,CAACrB,MAAM,CAACxE,IAAI,CAACkE;IACxB;IAEA,OAAO2B;AACT;AAGO,MAAM7Q,aAAa;IACxBT;IACAQ;IACAD;IACAmM;AACF;AAOO,eAAezM,aAAa4C,OAAe;IAChD,IAAI;QACF,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;QAChB,MAAME,SAASrC,UAAUsC,QAAQ,CAACH,YAAY,EAAE;QAEhD,sCAAsC;QACtC,MAAMwE,WAAW,MAAMkK,8BAA8B1O,SAASE;QAE9D,yBAAyB;QACzB,MAAMyO,iBAAiB,MAAMC,iBAAiBpK;QAC9C,MAAMqK,eAAeF,cAAc,CAAC,EAAE;QAEtC,2CAA2C;QAC3C,MAAMG,gBAAgB,MAAMC,sBAAsBvK,UAAUqK,aAAaG,QAAQ;QAEjF,0BAA0B;QAC1B,MAAMtO,SAAS,MAAMuO,sBAAsBjP,SAAS6O,aAAaG,QAAQ;QAEzE,OAAO;YACLA,UAAUH,aAAaG,QAAQ;YAC/BpP,YAAYiP,aAAajP,UAAU;YACnCkP,eAAeA,cAAc/K,GAAG,CAACmL,CAAAA,MAAQ,CAAA;oBACvC7E,MAAM6E,IAAI7E,IAAI;oBACdzK,YAAYsP,IAAItP,UAAU;gBAC5B,CAAA;YACAc,QAAQA,OAAOqD,GAAG,CAACuG,CAAAA,QAAU,CAAA;oBAC3BD,MAAMC,MAAMD,IAAI;oBAChB8E,WAAW7E,MAAM6E,SAAS;gBAC5B,CAAA;QACF;IACF,EAAE,OAAOxN,OAAO;QACd,MAAMC,eAAe;YACnBD,OAAOA,iBAAiBE,QAAQF,MAAMG,OAAO,GAAGC,OAAOJ;YACvDK,eAAehC,QAAQiC,MAAM;QAC/B;QACAC,cAAM,CAACP,KAAK,CAAC,gCAAgCC;QAC7C,MAAM,IAAIC,MAAM;IAClB;AACF;AAKO,eAAe3E,yBAAyB8C,OAAe;IAC5D,IAAI;QACF,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;QAChB,MAAMkD,YAAY7C,IAAI6C,SAAS,GAAGkB,GAAG,CAAC;QAEtC,4CAA4C;QAC5C,MAAMgL,mBAAmB,MAAMC,2BAA2BrP;QAE1D,kCAAkC;QAClC,MAAMY,UAAU,MAAM0O,yBAAyBtP;QAE/C,+CAA+C;QAC/C,MAAMuP,mBAAmBC,wBAAwBxP;QACjD,MAAMuB,WAAWkO,wBAAwBF;QAEzC,+BAA+B;QAC/B,MAAMG,YAAYC,4BAA4B3P;QAC9C,MAAM4P,eAAeC,sBAAsB7P;QAC3C,MAAM8P,UAAUC,cAAc/P;QAE9B,OAAO;YACLgQ,SAAS;gBACP7O,OAAOiO,iBAAiBjO,KAAK;gBAC7B8O,OAAOb,iBAAiBa,KAAK;gBAC7BrQ,YAAYwP,iBAAiBxP,UAAU;YACzC;YACAgB;YACAW;YACAgO;YACAG;YACAE;YACAE;QACF;IACF,EAAE,OAAOnO,OAAO;QACd,MAAMC,eAAe;YACnBD,OAAOA,iBAAiBE,QAAQF,MAAMG,OAAO,GAAGC,OAAOJ;YACvDK,eAAehC,QAAQiC,MAAM;QAC/B;QACAC,cAAM,CAACP,KAAK,CAAC,wCAAwCC;QACrD,MAAM,IAAIC,MAAM;IAClB;AACF;AAEA,sCAAsC;AAEtC,eAAe6M,8BAA8B1O,OAAe,EAAEE,MAAgB;IAC5E,kBAAkB;IAClB,MAAMwM,aAAa,IAAI5O,gBAAO,CAACG,KAAK;IACpCyO,WAAWpK,WAAW,CAACtC;IAEvB,kBAAkB;IAClB,MAAMmC,UAAUC,eAAM,CAACD,OAAO,CAACjC;IAC/B,MAAMmC,WAAWD,eAAM,CAACC,QAAQ,CAACnC;IAEjC,eAAe;IACf,MAAMG,MAAMC,IAAAA,mBAAG,EAACN;IAChB,MAAMkQ,MAAM7P,IAAImI,KAAK,GAAGpE,GAAG,CAAC;IAE5B,mBAAmB;IACnB,OAAO;QACLlE;QACAlC,OAAOkC,OAAO6D,GAAG,CAACC,CAAAA,IAAM,CAAA;gBAAE6E,MAAM7E;gBAAG7C,OAAOuL,WAAW1O,KAAK,CAACgG,GAAG;YAAG,CAAA;QACjEK,QAAQ;eAAIlC;eAAYE;SAAS,CAAC0B,GAAG,CAACO,CAAAA,KAAMA,GAAGC,IAAI,CAAC;QACpD2L;IACF;AACF;AAEA,eAAetB,iBAAiBpK,QAAa;IAC3C,qCAAqC;IACrC,MAAM8B,SAAS9H,kBAAkB2R,kBAAkB,CAAC3L,SAAStE,MAAM,CAACqE,IAAI,CAAC;IAEzE,OAAO+B,OACJvC,GAAG,CAACV,CAAAA,IAAM,CAAA;YACT2L,UAAU3L,EAAE4M,KAAK;YACjBrQ,YAAY6G,KAAK2J,GAAG,CAAC/M,EAAEgN,KAAK,IAAK,CAAA,IAAI5J,KAAK2J,GAAG,CAAC/M,EAAEgN,KAAK,EAAG,yBAAyB;YAA5B;QACvD,CAAA,GACCvH,IAAI,CAAC,CAACjC,GAAGC,IAAMA,EAAElH,UAAU,GAAGiH,EAAEjH,UAAU;AAC/C;AAEA,eAAemP,sBACbvK,QAAa,EACbqK,YAAoB;IAEpB,2CAA2C;IAC3C,MAAMC,gBAAgBwB,iBAAiBzB;IAEvC,4CAA4C;IAC5C,MAAMpJ,UAAUqJ,cAAc/K,GAAG,CAACmL,CAAAA;QAChC,MAAMtP,aAAa2Q,+BAA+B/L,UAAU0K;QAC5D,OAAO;YACL7E,MAAM6E;YACNtP;QACF;IACF;IAEA,OAAO6F,QACJqD,IAAI,CAAC,CAACjC,GAAGC,IAAMA,EAAElH,UAAU,GAAGiH,EAAEjH,UAAU,EAC1CmJ,KAAK,CAAC,GAAG,IAAI,sBAAsB;AACxC;AAEA,SAASuH,iBAAiBtB,QAAgB;IACxC,8CAA8C;IAC9C,MAAMwB,iBAA2C;QAC/CC,WAAW;YAAC;YAAgB;YAAS;YAAW;YAAe;SAAW;QAC1EC,WAAW;YAAC;YAAe;YAAkB;YAAY;YAAQ;SAAc;QAC/EC,WAAW;YAAC;YAAY;YAAW;YAAc;YAAc;SAAO;QACtEC,SAAS;YAAC;YAAoB;YAAqB;YAAiB;SAAW;IACjF;IAEA,OAAOJ,cAAc,CAACxB,SAAS,IAAI,EAAE;AACvC;AAEA,SAASuB,+BAA+B/L,QAAa,EAAEqM,WAAmB;IACxE,wDAAwD;IACxD,MAAMC,gBAAgBC,0BAA0BF;IAChD,MAAMG,cAAcxM,SAAStE,MAAM,CAACkD,MAAM,CAAC,CAACgK,QAC1C0D,cAAc3C,IAAI,CAACtF,CAAAA,OAAQuE,MAAMhN,WAAW,GAAG8I,QAAQ,CAACL,QACxD5G,MAAM;IAER,OAAOwE,KAAKC,GAAG,CAAC,GAAGsK,cAAcvK,KAAK8C,GAAG,CAAC,GAAG/E,SAAStE,MAAM,CAAC+B,MAAM;AACrE;AAEA,SAAS8O,0BAA0BF,WAAmB;IACpD,6CAA6C;IAC7C,MAAMI,UAAoC;QACxC,gBAAgB;YAAC;YAAU;YAAS;YAAQ;YAAc;SAAY;QACtE,SAAS;YAAC;YAAS;YAAc;YAAY;YAAQ;SAAQ;QAC7D,WAAW;YAAC;YAAW;YAAQ;YAAW;YAAS;SAAO;IAE5D;IAEA,OAAOA,OAAO,CAACJ,YAAY,IAAI,EAAE;AACnC;AAEA,mDAAmD;AAEnD,eAAexB,2BAA2BrP,OAAe;IAKvD,qCAAqC;IACrC,MAAMkR,aAAahT,kBAAkBsC,OAAO,CAACR,SAASmB,KAAK;IAC3D,MAAMgQ,eAAe,IAAIC,0BAAgB,CAAC,WAAWC,sBAAa,EAAE,SAASC,YAAY,CACvFzT,UAAUsC,QAAQ,CAACH,YAAY,EAAE;IAGnC,8BAA8B;IAC9B,MAAMuR,gBAAgB,AAACL,aAAa,MAAQC,eAAe;IAE3D,0CAA0C;IAC1C,MAAMvR,aAAa,IAAI6G,KAAK+K,GAAG,CAACN,aAAaC,gBAAgB;IAE7D,OAAO;QACLhQ,OAAOoQ;QACPtB,OAAOsB,gBAAgB,MAAM,aAAaA,gBAAgB,CAAC,MAAM,aAAa;QAC9E3R;IACF;AACF;AAEA,eAAe0P,yBAAyBtP,OAAe;IAQrD,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;IAChB,MAAMY,UAOD,EAAE;IAEP,4CAA4C;IAC5C,MAAM0N,cAAcjO,IAAI8D,KAAK,CAAC,UAAUC,GAAG,CAAC;IAE5C,KAAK,MAAMsB,UAAU4I,YAAa;QAChC,uCAAuC;QACvC,MAAM3I,oBAAoB3F,QACvBmD,KAAK,CAAC,UACNC,MAAM,CAACC,CAAAA,IAAKA,EAAEjD,WAAW,GAAG8I,QAAQ,CAACxD,OAAOtF,WAAW;QAE1D,IAAIuF,kBAAkB1D,MAAM,GAAG,GAAG;YAChC,uCAAuC;YACvC,MAAMkH,aAAa,MAAMsI,QAAQC,GAAG,CAClC/L,kBAAkB5B,GAAG,CAACV,CAAAA,IAAKgM,2BAA2BhM;YAGxD,2BAA2B;YAC3B,MAAMsO,WAAWxI,WAAW1F,MAAM,CAAC,CAACC,KAAKL,IAAMK,MAAML,EAAElC,KAAK,EAAE,KAAKgI,WAAWlH,MAAM;YAEpFrB,QAAQgI,IAAI,CAAC;gBACXlD;gBACAvH,WAAW;oBACTgD,OAAOwQ;oBACP1B,OAAO0B,WAAW,MAAM,aAAaA,WAAW,CAAC,MAAM,aAAa;gBACtE;gBACAC,UAAUjM,kBAAkBoD,KAAK,CAAC,GAAG,GAAG,iBAAiB;YAC3D;QACF;IACF;IAEA,OAAOnI;AACT;AAEA,SAAS4O,wBAAwBxP,OAAe;IAC9C,MAAMkD,YAAYlD,QAAQmD,KAAK,CAAC;IAChC,uBAAuB;IACvB,MAAM0O,iBAAyC,CAAC;IAChDC,OAAOC,IAAI,CAAClT,uBAAuB6J,OAAO,CAACuH,CAAAA;QACzC4B,cAAc,CAAC5B,MAAM,GAAG;IAC1B;IAEA,KAAK,MAAMhH,YAAY/F,UAAW;QAChC,MAAM8O,kBAAkBtT,kBAAkByR,kBAAkB,CAAClH;QAC7D+I,gBAAgBtJ,OAAO,CAACuJ,CAAAA;YACtB,IAAIJ,cAAc,CAACI,EAAEhC,KAAK,CAAC,KAAKiC,WAAW;gBACzC,MAAMC,OAAO1L,KAAK2J,GAAG,CAAC6B,EAAE5B,KAAK,IAAK,CAAA,IAAI5J,KAAK2J,GAAG,CAAC6B,EAAE5B,KAAK,CAAA;gBACtDwB,cAAc,CAACI,EAAEhC,KAAK,CAAC,IAAIkC;YAC7B;QACF;IACF;IAEA,YAAY;IACZ,MAAMC,QAAQN,OAAOO,MAAM,CAACR,gBAAgBpO,MAAM,CAAC,CAACC,KAAKiK,IAAMjK,MAAMiK,GAAG;IACxE,IAAIyE,QAAQ,GAAG;QACbN,OAAOC,IAAI,CAACF,gBAAgBnJ,OAAO,CAACwB,CAAAA;YAClC2H,cAAc,CAAC3H,EAAE,IAAIkI;QACvB;IACF;IAEA,OAAOP;AACT;AAEA,SAASpC,wBAAwB6C,QAAgC;IAC/D,MAAMC,OAA+B;QACnCzT,KAAK;QACLC,SAAS;QACTC,OAAO;QACPC,MAAM;QACNC,UAAU;QACVC,OAAO;QACPC,cAAc;QACdC,SAAS;IACX;IAEAyS,OAAOU,OAAO,CAACF,UAAU5J,OAAO,CAAC,CAAC,CAACuH,OAAO9O,MAAM;QAC9C,MAAMsR,SAAS1S,oBAAoB,CAACkQ,MAA2C;QAC/E,IAAIwC,QAAQ;YACVF,IAAI,CAACE,OAAO,IAAItR;QAClB,OAAO,IAAI,AAACoR,IAAY,CAACtC,MAAM,KAAKiC,WAAW;YAC7C,gCAAgC;YAC/BK,IAAY,CAACtC,MAAM,IAAI9O;QAC1B;IACF;IAEA,OAAOoR;AACT;AAEA,SAAS5C,4BAA4B3P,OAAe;IAClD,MAAM0S,eAAe;QAAC;QAAQ;QAAa;QAAc;QAAW;KAAa;IACjF,MAAMC,eAAe,AAAC3S,CAAAA,QAAQmE,KAAK,CAAC,SAAS,EAAE,AAAD,EAAGlC,MAAM;IACvD,MAAM2Q,YAAY,AAAC5S,CAAAA,QAAQmE,KAAK,CAAC,iBAAiB,EAAE,AAAD,EAAGlC,MAAM;IAC5D,MAAM4Q,mBAAmBH,aAAajP,MAAM,CAC1C,CAAC0C,OAAO0B,OAAS1B,QAAQ,AAACnG,CAAAA,QAAQI,WAAW,GAAG+D,KAAK,CAAC,IAAI6J,OAAOnG,MAAM,SAAS,EAAE,AAAD,EAAG5F,MAAM,EAC1F;IAGF,OAAOwE,KAAKC,GAAG,CAAC,GAAImM,mBAAmB,MAAMF,eAAe,MAAMC,YAAY;AAChF;AAEA,SAAS/C,sBAAsB7P,OAAe;IAC5C,MAAMK,MAAMC,IAAAA,mBAAG,EAACN;IAChB,MAAM2H,QAAQtH,IAAImI,KAAK,GAAGpE,GAAG,CAAC;IAE9B,8BAA8B;IAC9B,MAAM0O,mBAAmBzS,IAAI8D,KAAK,CAAC,iCAAiCC,GAAG,CAAC,SAASnC,MAAM;IACvF,MAAM8Q,WAAW1S,IAAI8D,KAAK,CAAC,uCAAuCC,GAAG,CAAC,SAASnC,MAAM;IACrF,MAAM+Q,aAAa3S,IAAI8D,KAAK,CAAC,cAAcC,GAAG,CAAC,SAASnC,MAAM;IAE9D,OAAOwE,KAAKC,GAAG,CAAC,GAAG,AAACoM,CAAAA,mBAAmBC,WAAW,IAAIC,UAAS,IAAKrL,MAAM1F,MAAM;AAClF;AAEA,SAAS8N,cAAc/P,OAAe;IACpC,MAAMiT,oBAAoB;QACxB,yCAAyC;QACzCjT,QAAQmE,KAAK,CAAC;QACd,eAAe;QACfnE,QAAQmE,KAAK,CAAC;QACd,wCAAwC;QACxCnE,QAAQmE,KAAK,CAAC;QACd,+BAA+B;QAC/BnE,QAAQmE,KAAK,CAAC;KACf;IAED,MAAM+O,iBAAiBD,kBAAkB7P,MAAM,CAAC8I,CAAAA,IAAKA,MAAM,MAAMjK,MAAM;IACvE,MAAMrC,aAAa6G,KAAKC,GAAG,CAAC,GAAGwM,iBAAiB;IAEhD,OAAO;QACLC,UAAUvT,aAAa;QACvBA;IACF;AACF;AAEA,mCAAmC;AACnC,SAASwT;IACP,2BAA2B;IAC3BtB,OAAOU,OAAO,CAAC3T,uBAAuB6J,OAAO,CAAC,CAAC,CAACrC,SAASuL,SAAS;QAChEA,SAASlJ,OAAO,CAAC2K,CAAAA;YACf3U,kBAAkB4D,WAAW,CAAC+Q,SAAShN;QACzC;IACF;IACA3H,kBAAkB4U,KAAK;IAEvB,gDAAgD;IAChD,MAAMC,oBAAoB;QACxB9C,WAAW;YAAC;YAAY;YAAa;YAAiB;YAAS;SAAS;QACxEC,WAAW;YAAC;YAAY;YAAY;YAAU;YAAQ;SAAa;QACnEC,WAAW;YAAC;YAAW;YAAe;YAAS;YAAW;SAAU;QACpEC,SAAS;YAAC;YAAQ;YAAc;YAAW;YAAS;SAAa;IACnE;IAEAkB,OAAOU,OAAO,CAACe,mBAAmB7K,OAAO,CAAC,CAAC,CAACsG,UAAU4C,SAAS;QAC7DA,SAASlJ,OAAO,CAAC2K,CAAAA;YACf7U,kBAAkB8D,WAAW,CAAC+Q,SAASrE;QACzC;IACF;IACAxQ,kBAAkB8U,KAAK;AACzB;AAEA,2CAA2C;AAC3CF;AAGO,MAAMnW,uBAAuB;IAClCG;IACAF;IACA6S;IACAF;AACF;AAGO,MAAMvS,8BAA8B;IACzCE;IACAD;IACA,GAAGK,UAAU;IACb,GAAGX,oBAAoB;IACvB,GAAGI,0BAA0B;AAC/B;AAEA,wBAAwB;AAExB,eAAe4R,sBAAsBjP,OAAe,EAAEgP,QAAgB;IACpE,MAAM3O,MAAMC,IAAAA,mBAAG,EAACN;IAChB,MAAM0M,aAAa,IAAI5O,gBAAO,CAACG,KAAK;IAEpC,4CAA4C;IAC5C,MAAMuV,eAAeC,qBAAqBzE;IAC1CwE,aAAa9K,OAAO,CAACrI,CAAAA,MAAOqM,WAAWpK,WAAW,CAACjC;IAEnD,sBAAsB;IACtBqM,WAAWpK,WAAW,CAACtC;IAEvB,qCAAqC;IACrC,MAAMwI,QAAQkE,WAAWjE,SAAS,CAAC;IACnC,MAAM/H,SAAS8H,MACZO,KAAK,CAAC,GAAG,IACThF,GAAG,CAAC8E,CAAAA,OAAS,CAAA;YACZwB,MAAMxB,KAAKA,IAAI;YACfsG,WAAWtG,KAAK7K,KAAK;QACvB,CAAA;IAEF,OAAO0C;AACT;AAEA,SAAS+S,qBAAqBzE,QAAgB;IAC5C,qCAAqC;IACrC,MAAMwE,eAAyC;QAC7C/C,WAAW;YACT;YACA;YACA;SACD;QACDC,WAAW;YACT;YACA;YACA;SACD;QACDC,WAAW;YACT;YACA;YACA;SACD;QACDC,SAAS;YACP;YACA;YACA;SACD;IACH;IAEA,OAAO4C,YAAY,CAACxE,SAAS,IAAI,EAAE;AACrC"}