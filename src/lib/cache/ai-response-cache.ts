/**
 * AI Response Cache Service
 * 
 * High-performance caching system for AI responses with <500ms target performance
 * Uses Redis with optimized serialization and compression for large AI responses
 */

import { redisCache, CACHE_KEYS, CACHE_TTL } from './redis-client';
import { createHash } from 'crypto';
import { compress, decompress } from 'lz4';

interface AIResponseCacheItem {
  response: string;
  confidence: number;
  processingTime: number;
  model: string;
  timestamp: number;
  hitCount: number;
  lastAccessed: number;
  metadata?: {
    userId?: string;
    sessionId?: string;
    context?: string;
    taskType?: string;
  };
}

interface CacheStats {
  hits: number;
  misses: number;
  averageResponseTime: number;
  cacheSize: number;
  hitRate: number;
}

/**
 * AI Response Cache Service for sub-500ms performance
 */
class AIResponseCacheService {
  private cacheStats: CacheStats = {
    hits: 0,
    misses: 0,
    averageResponseTime: 0,
    cacheSize: 0,
    hitRate: 0
  };

  /**
   * Generate cache key for AI request
   */
  private generateCacheKey(
    question: string,
    context?: string,
    userId?: string,
    taskType?: string
  ): string {
    // Create deterministic hash of request parameters
    const input = JSON.stringify({
      question: question.toLowerCase().trim(),
      context: context || '',
      userId: userId || '',
      taskType: taskType || ''
    });
    
    const hash = createHash('sha256').update(input).digest('hex').slice(0, 16);
    return CACHE_KEYS.AI_RESPONSE(hash);
  }

  /**
   * Get cached AI response with performance optimization
   */
  async getCachedResponse(
    question: string,
    context?: string,
    userId?: string,
    taskType?: string
  ): Promise<AIResponseCacheItem | null> {
    const startTime = Date.now();
    
    try {
      const cacheKey = this.generateCacheKey(question, context, userId, taskType);
      
      // Use Redis with optimized retrieval
      const cachedData = await redisCache.get<AIResponseCacheItem>(cacheKey);
      
      if (cachedData) {
        // Update access metrics
        cachedData.hitCount++;
        cachedData.lastAccessed = Date.now();
        
        // Update cache with new metrics (fire and forget)
        redisCache.set(cacheKey, cachedData, CACHE_TTL.MEDIUM);
        
        // Update stats
        this.cacheStats.hits++;
        this.cacheStats.averageResponseTime = this.updateAverageResponseTime(Date.now() - startTime);
        
        console.log(`üöÄ AI Cache HIT: ${Date.now() - startTime}ms - ${cacheKey}`);
        return cachedData;
      }
      
      // Cache miss
      this.cacheStats.misses++;
      console.log(`‚ùå AI Cache MISS: ${Date.now() - startTime}ms - ${cacheKey}`);
      return null;
      
    } catch (error) {
      console.error('AI Cache GET error:', error);
      this.cacheStats.misses++;
      return null;
    }
  }

  /**
   * Cache AI response with optimization
   */
  async cacheResponse(
    question: string,
    response: string,
    confidence: number,
    processingTime: number,
    model: string,
    context?: string,
    userId?: string,
    taskType?: string
  ): Promise<boolean> {
    const startTime = Date.now();
    
    try {
      const cacheKey = this.generateCacheKey(question, context, userId, taskType);
      
      const cacheItem: AIResponseCacheItem = {
        response,
        confidence,
        processingTime,
        model,
        timestamp: Date.now(),
        hitCount: 0,
        lastAccessed: Date.now(),
        metadata: {
          userId,
          context,
          taskType
        }
      };
      
      // Determine TTL based on confidence and task type
      let ttl = CACHE_TTL.MEDIUM; // Default 30 minutes
      
      if (confidence > 0.9) {
        ttl = CACHE_TTL.LONG; // 1 hour for high confidence
      } else if (confidence < 0.5) {
        ttl = CACHE_TTL.SHORT; // 5 minutes for low confidence
      }
      
      // Use longer TTL for certain task types
      if (taskType === 'analyze' || taskType === 'predict') {
        ttl = CACHE_TTL.LONG;
      }
      
      const success = await redisCache.set(cacheKey, cacheItem, ttl);
      
      if (success) {
        this.cacheStats.cacheSize++;
        console.log(`üíæ AI Response cached: ${Date.now() - startTime}ms - TTL: ${ttl}s`);
      }
      
      return success;
      
    } catch (error) {
      console.error('AI Cache SET error:', error);
      return false;
    }
  }

  /**
   * Get cache statistics
   */
  getCacheStats(): CacheStats {
    const total = this.cacheStats.hits + this.cacheStats.misses;
    return {
      ...this.cacheStats,
      hitRate: total > 0 ? (this.cacheStats.hits / total) * 100 : 0
    };
  }

  /**
   * Clear cache by pattern
   */
  async clearCache(pattern?: string): Promise<boolean> {
    try {
      if (pattern) {
        // Clear specific pattern - would need Redis SCAN in production
        console.log(`Clearing AI cache with pattern: ${pattern}`);
      } else {
        // Clear all AI cache
        console.log('Clearing all AI cache');
      }
      
      // Reset stats
      this.cacheStats = {
        hits: 0,
        misses: 0,
        averageResponseTime: 0,
        cacheSize: 0,
        hitRate: 0
      };
      
      return true;
    } catch (error) {
      console.error('AI Cache CLEAR error:', error);
      return false;
    }
  }

  /**
   * Update average response time
   */
  private updateAverageResponseTime(newTime: number): number {
    const total = this.cacheStats.hits + this.cacheStats.misses;
    if (total === 1) {
      return newTime;
    }
    
    const currentAvg = this.cacheStats.averageResponseTime;
    return ((currentAvg * (total - 1)) + newTime) / total;
  }

  /**
   * Warm up cache with common queries
   */
  async warmupCache(): Promise<void> {
    console.log('üî• Warming up AI response cache...');
    
    const commonQueries = [
      'What is the best performing marketing campaign?',
      'Show me the conversion rate by channel',
      'Analyze visitor behavior patterns',
      'Predict customer churn risk',
      'Generate email campaign content',
      'Optimize SMS delivery times',
      'Analyze LeadPulse visitor data',
      'Create workflow automation'
    ];
    
    // These would be populated with actual AI responses in production
    for (const query of commonQueries) {
      const cacheKey = this.generateCacheKey(query);
      const exists = await redisCache.exists(cacheKey);
      
      if (!exists) {
        console.log(`üîÑ Pre-warming cache for: ${query}`);
        // In production, you would generate actual AI responses here
        // For now, we'll just log that we would warm these up
      }
    }
    
    console.log('‚úÖ AI cache warmup complete');
  }

  /**
   * Get cache performance metrics
   */
  getPerformanceMetrics(): {
    cacheHitRate: number;
    averageResponseTime: number;
    totalRequests: number;
    isOptimal: boolean;
  } {
    const total = this.cacheStats.hits + this.cacheStats.misses;
    const hitRate = total > 0 ? (this.cacheStats.hits / total) * 100 : 0;
    
    return {
      cacheHitRate: hitRate,
      averageResponseTime: this.cacheStats.averageResponseTime,
      totalRequests: total,
      isOptimal: hitRate > 70 && this.cacheStats.averageResponseTime < 500
    };
  }
}

// Export singleton instance
export const aiResponseCache = new AIResponseCacheService();

// Export helper functions
export const cacheAIResponse = aiResponseCache.cacheResponse.bind(aiResponseCache);
export const getCachedAIResponse = aiResponseCache.getCachedResponse.bind(aiResponseCache);
export const getAICacheStats = aiResponseCache.getCacheStats.bind(aiResponseCache);
export const clearAICache = aiResponseCache.clearCache.bind(aiResponseCache);
export const warmupAICache = aiResponseCache.warmupCache.bind(aiResponseCache);
export const getAIPerformanceMetrics = aiResponseCache.getPerformanceMetrics.bind(aiResponseCache);