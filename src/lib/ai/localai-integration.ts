/**
 * LocalAI Integration - Drop-in replacement for OpenAI
 * ===================================================
 * 
 * Provides identical interface to OpenAI but runs locally
 * - Same API endpoints and response formats
 * - Complete privacy - data never leaves your server
 * - No API costs - unlimited usage
 * - Function calling support
 */

interface OpenAIMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface LocalAIResponse {
  answer: string;
  usage?: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}

export class LocalAIIntegration {
  private baseURL: string;
  private apiKey = 'localai'; // LocalAI doesn't require real keys
  private defaultModel: string;

  constructor() {
    this.baseURL = process.env.LOCALAI_API_BASE_URL || 'http://localhost:8080/v1';
    this.defaultModel = process.env.LOCALAI_DEFAULT_MODEL || 'gpt-3.5-turbo';
    
    // Validate LocalAI is available
    this.validateConnection();
  }

  private async validateConnection(): Promise<void> {
    try {
      const response = await fetch(`${this.baseURL}/models`, {
        method: 'GET',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        signal: AbortSignal.timeout(5000) // 5 second timeout
      });

      if (!response.ok) {
        console.warn('LocalAI connection check failed, but will continue');
      }
    } catch (error) {
      console.warn('LocalAI may not be running:', error instanceof Error ? error.message : String(error));
    }
  }

  async generateResponse(
    question: string, 
    context?: string,
    conversationHistory: OpenAIMessage[] = [],
    options: {
      temperature?: number;
      maxTokens?: number;
      model?: string;
    } = {}
  ): Promise<LocalAIResponse> {
    const systemPrompt = this.buildSystemPrompt(context);
    
    const messages: OpenAIMessage[] = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory.slice(-6), // Keep last 6 messages for context
      { role: 'user', content: question }
    ];

    try {
      const response = await fetch(`${this.baseURL}/chat/completions`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: options.model || this.defaultModel,
          messages,
          max_tokens: options.maxTokens || 1000,
          temperature: options.temperature || 0.7,
          presence_penalty: 0.1,
          frequency_penalty: 0.1,
        }),
      });

      if (!response.ok) {
        const errorData = await response.json().catch(() => ({ error: 'Unknown error' }));
        throw new Error(`LocalAI API error: ${errorData.error?.message || response.statusText}`);
      }

      const data = await response.json();
      
      return {
        answer: data.choices[0]?.message?.content || 'I apologize, but I was unable to generate a response.',
        usage: data.usage
      };
    } catch (error) {
      console.error('LocalAI API call failed:', error);
      
      // Graceful fallback to basic response
      return {
        answer: `I'm having trouble connecting to the local AI service. This might be because LocalAI is not running. Here's what I can suggest about "${question}": Please ensure LocalAI is properly configured and running.`,
        usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 }
      };
    }
  }

  private buildSystemPrompt(context?: string): string {
    const basePrompt = `You are MarketSage AI, an intelligent assistant specialized in marketing automation for African businesses. You help with:

üéØ Email Marketing: Campaign creation, segmentation, personalization
üì± SMS & WhatsApp: Multi-channel messaging strategies  
üîÑ Workflow Automation: Customer journey optimization
üìä Analytics: Performance insights and recommendations
üåç African Market Focus: Cultural sensitivity, local insights, regional best practices

Key principles:
- Be helpful, accurate, and conversational
- Provide actionable advice specific to African markets
- Reference MarketSage features when relevant
- Ask clarifying questions when needed
- Keep responses concise but comprehensive`;

    if (context) {
      return `${basePrompt}\n\nContext: ${context}\n\nRespond naturally and helpfully.`;
    }

    return basePrompt;
  }

  // Additional methods to match OpenAI interface
  async generateEmbeddings(texts: string[]): Promise<number[][]> {
    try {
      const response = await fetch(`${this.baseURL}/embeddings`, {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'text-embedding-ada-002', // LocalAI embedding model
          input: texts,
        }),
      });

      if (!response.ok) {
        throw new Error(`LocalAI embeddings error: ${response.statusText}`);
      }

      const data = await response.json();
      return data.data.map((item: any) => item.embedding);
    } catch (error) {
      console.error('LocalAI embeddings failed:', error);
      // Return zero vectors as fallback
      return texts.map(() => new Array(1536).fill(0));
    }
  }

  async analyzeText(
    text: string,
    analysisType: 'sentiment' | 'summary' | 'keywords' | 'classification'
  ): Promise<any> {
    const prompts = {
      sentiment: `Analyze the sentiment of this text and return a JSON with sentiment (positive/negative/neutral) and confidence (0-1): "${text}"`,
      summary: `Provide a concise summary of this text: "${text}"`,
      keywords: `Extract the main keywords from this text as a JSON array: "${text}"`,
      classification: `Classify this text into relevant categories and return as JSON: "${text}"`
    };

    const response = await this.generateResponse(prompts[analysisType]);
    
    try {
      // Try to parse as JSON if it looks like JSON
      if (response.answer.trim().startsWith('{') || response.answer.trim().startsWith('[')) {
        return JSON.parse(response.answer);
      }
      return { result: response.answer };
    } catch {
      return { result: response.answer };
    }
  }

  async evaluateApproach(
    approach: string,
    criteria: string[]
  ): Promise<{
    score: number;
    feedback: string[];
  }> {
    const response = await this.generateResponse(
      `Evaluate this approach: ${approach}\nCriteria: ${criteria.join(', ')}`,
      'You are an expert evaluator. Provide a detailed assessment.',
      [],
      { temperature: 0.3 }
    );

    try {
      const evaluation = JSON.parse(response.answer);
      return {
        score: evaluation.score || 0.5,
        feedback: evaluation.feedback || []
      };
    } catch {
      return {
        score: 0.5,
        feedback: ['Evaluation completed using LocalAI']
      };
    }
  }

  // Health check method
  async healthCheck(): Promise<{ status: 'healthy' | 'unhealthy'; message: string; models?: string[] }> {
    try {
      const response = await fetch(`${this.baseURL}/models`, {
        method: 'GET',
        headers: {
          'Authorization': `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json',
        },
        signal: AbortSignal.timeout(3000)
      });

      if (response.ok) {
        const data = await response.json();
        const models = data.data?.map((model: any) => model.id) || [];
        return {
          status: 'healthy',
          message: `LocalAI is running with ${models.length} models available`,
          models
        };
      } else {
        return {
          status: 'unhealthy',
          message: `LocalAI responded with status ${response.status}`
        };
      }
    } catch (error) {
      return {
        status: 'unhealthy',
        message: `Cannot connect to LocalAI: ${error instanceof Error ? error.message : String(error)}`
      };
    }
  }
}

// Export singleton instance
export const localAI = new LocalAIIntegration(); 